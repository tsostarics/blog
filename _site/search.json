[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "On Helmert Coding\n\n\n\n\n\n\nR\n\n\n\nThis post discusses the behavior of Helmert contrasts.\n\n\n\n\n\nJan 14, 2024\n\n\nThomas Sostarics\n\n\n\n\n\n\n\n\n\n\n\n\nPulse Labeling\n\n\n\n\n\n\nR\n\n\nPraat\n\n\n\nThis post discusses using non-equi joins to label pitch pulses by an interval on a TextGrid.\n\n\n\n\n\nJan 25, 2023\n\n\nThomas Sostarics\n\n\n\n\n\n\n\n\n\n\n\n\nContrastable\n\n\n\n\n\n\nR pkg\n\n\n\nThis post gives an overview of an R package I’ve written. The contrastable package provides a tidy approach to contrast coding for regression analyses.\n\n\n\n\n\nJul 13, 2022\n\n\nThomas Sostarics\n\n\n\n\n\n\n\n\n\n\n\n\nNote Frequencies\n\n\n\n\n\n\nReference\n\n\n\nA quick reference for musical notes and their corresponding frequencies and wavelengths.\n\n\n\n\n\nJul 13, 2022\n\n\nThomas Sostarics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/contrastable/index.html",
    "href": "posts/contrastable/index.html",
    "title": "Contrastable",
    "section": "",
    "text": "I’ve been working on a package called contrastable on and off for the past year or so. The package’s goal is to provide a tidy approach to setting factor contrasts for regression analysis. While this can be done with repeated contrasts<- calls, this workflow is tedious when working with multiple factors and especially error-prone when manually specifying contrast matrices to use. In this latter case, the user would need to be careful to specify the correct fractions in the correct order with the correct signs, which can be a lot to keep track of. These issues quickly become apparent when the number of factor levels is greater than 2. In this post I will:"
  },
  {
    "objectID": "posts/contrastable/index.html#contrasts-overview",
    "href": "posts/contrastable/index.html#contrasts-overview",
    "title": "Contrastable",
    "section": "Contrasts overview",
    "text": "Contrasts overview\nContrast coding refers to assigning numeric values to levels of a categorical variable for use in regression analyses. Depending on the numbers used, different comparisons can be made between the group means of a variable. These comparisons can correspond to particular null hypotheses that a researcher might have, and particular combinations of numbers can encode high-level questions like “Are there differences between levels when compared to a common reference level?” or “Does each level differ from the levels that came before it?” Critically, the contrasts used don’t impact the model fit but do impact the coefficient estimates that are used to make inferences about the data. For example, you might conclude that there’s an overall effect of some factor when in reality the effect (shown by the coefficient estimate) is an effect that only holds for one particular group!\nConsider an example where you have two groups of listeners where English is their native (L1) or non-native (L2) language. You might be interested in whether reading times are slower or faster in two different syntactic conditions, such as active vs passive constructions. Two possible research questions might be whether there’s an main effect of syntax on reading times on the one hand or whether there’s a simple effect such that reading times in the passive construction are only slower for L2 speakers.1 These are similar, but different, research questions and more importantly, the interpretation of one coefficient depends on how other variables are coded. Many researchers realize (or are starting to at least) that the default2 “0/1 contrasts” (aka treatment or dummy coding) will only give them the simple effect of structure, but if what you’re interested in is that main effect, then your statistics has not yet answered your question! To rectify this, researchers will opt for “the +.5/-.5 contrasts” to obtain main effects.\n\n\n\n\n\n\nWarning: Naming inconsistency\n\n\n\nThe name for this contrast scheme is not consistent, especially in the 2-level case where the values are +.5/-.5. I’ve seen it called sum coding, simple coding, effects coding, scaled sum coding, helmert coding, difference coding, contrast coding, sum-to-zero coding, and +.5/-.5 coding. See Brehm and Alday (2022) for concerns about transparent descriptions of contrast coding schemes.\n\n\nWhile researchers may differ on what they call +.5/-.5, for 2 levels the result is nonetheless the same. But, a researcher using “helmert coding” and a researcher using “scaled sum coding” for a factor with 3 or more levels will be using very different contrast matrices, and thus address very different research questions about the data. Let’s use some functions from contrastable to look at how these contrast matrices differ.\n\nlibrary(contrastable)\n\n\n2 levels3 levels5 levels\n\n\n\nhelmert_code(2) |> MASS::fractions()\n\n     [,1]\n[1,]  1/2\n[2,] -1/2\n\nscaled_sum_code(2) |> MASS::fractions()\n\n  [,1]\n1  1/2\n2 -1/2\n\n\n\n\n\nhelmert_code(3) |> MASS::fractions()\n\n     [,1] [,2]\n[1,]  2/3    0\n[2,] -1/3  1/2\n[3,] -1/3 -1/2\n\nscaled_sum_code(3) |> MASS::fractions()\n\n  [,1] [,2]\n1  2/3 -1/3\n2 -1/3  2/3\n3 -1/3 -1/3\n\n\n\n\n\nhelmert_code(5) |> MASS::fractions()\n\n     [,1] [,2] [,3] [,4]\n[1,]  4/5    0    0    0\n[2,] -1/5  3/4    0    0\n[3,] -1/5 -1/4  2/3    0\n[4,] -1/5 -1/4 -1/3  1/2\n[5,] -1/5 -1/4 -1/3 -1/2\n\nscaled_sum_code(5) |> MASS::fractions()\n\n  [,1] [,2] [,3] [,4]\n1  4/5 -1/5 -1/5 -1/5\n2 -1/5  4/5 -1/5 -1/5\n3 -1/5 -1/5  4/5 -1/5\n4 -1/5 -1/5 -1/5  4/5\n5 -1/5 -1/5 -1/5 -1/5\n\n\n\n\n\nNote that I use the term scaled sum coding for the “pairwise comparisons with main effects” contrast scheme. I opt for this term for three reasons.\n\nFirst, I see sum coding used more frequently in statistics and econometrics to refer to +1/-1; this is also what contr.sum in R returns.\nSecond, the salient part of going from sum coding to scaled sum coding, especially in the 2-level case, is that there’s some kind of division or scaling operation involved; I frequently see people use contr.sum(2)/2, although importantly contr.sum(3)/3 does not yield the expected result.\nThird, “simple” coding is counterintuitive to me since we’re trying to avoid “simple effects;” “effects coding” and “contrast coding” are largely meaningless as all coding schemes will encode some kind of effect, and setting any contrast matrix is an instance of contrast coding.\n\nSo, for the researcher trying to remember “I need to use those contrasts where they’re divided to get the main effects”, it (to me) seems easy to reach for a tool where scaled is in the name and is clearly distinguished from sum coding. 3"
  },
  {
    "objectID": "posts/contrastable/index.html#typical-approach-to-contrast-coding",
    "href": "posts/contrastable/index.html#typical-approach-to-contrast-coding",
    "title": "Contrastable",
    "section": "Typical approach to contrast coding",
    "text": "Typical approach to contrast coding\nTypically when I see people in Linguistics set contrasts, they do something like the following, using the palmerpenguins dataset as an example.\n\n\nCode\nlibrary(dplyr)          # Data wrangling\nlibrary(purrr)          # Mapping functions\nlibrary(palmerpenguins) # Dataset\npenguins_with_contrasts <- penguins\n\n# Default treatment/dummy coding for a 2 and 3 level factor\ncontrasts(penguins_with_contrasts$sex)\n\n\n       male\nfemale    0\nmale      1\n\n\nCode\ncontrasts(penguins_with_contrasts$species)\n\n\n          Chinstrap Gentoo\nAdelie            0      0\nChinstrap         1      0\nGentoo            0      1\n\n\nCode\n# Easy enough for 2 levels, -contr.sum(2)/2 is also used a lot\ncontrasts(penguins_with_contrasts$sex) <- c(-.5, .5) \n\n# Not so fun for three levels!\ncontrasts(penguins_with_contrasts$species) <- matrix(c(-1/3, 2/3, -1/3,\n                                                       -1/3, -1/3, 2/3),\n                                                     nrow = 3)\n\n\nThe chance of making a mistake increases when including more and more categorical variables. Catching these mistakes can be very difficult, in part because this workflow erases the labels in the regression output. This means you have to keep track of what 1 and 2 in the regression coefficients correspond to.\n\n\n\n\n\n\nNote: Column/Comparison/Coefficient names\n\n\n\nWhile the dimnames argument can be used to set the labels, anecdotally I rarely see people use this in their analyses when perusing code on the osf. Winter (2019, 127) notes that “Using the ‘1’ after the predictor name is a notational convention for representing the slopes of sum-coded predictors in R” but this is slightly incorrect; in the absence of dimnames being set, R will use the numeric indices of the contrast matrix’s columns (no matter what the scheme is).\n\n\nBelow, the two sets of coefficients represent pairwise comparisons to the Adelie baseline, but the intercepts differ due to how the contrasts are set, with the first using treatment coding and the second using scaled sum coding. I’ll start with a case that only considers the categorical variable, but will include an additional continuous independent variable later on.\n\n\nCode\n# Compare the default treatment coding with the penguins dataset\n# with the contrasts we specified in penguins_with_contrasts\ntreatment_coefs <- coef(lm(bill_length_mm ~ species,\n                           data = penguins))    \nscaledsum_coefs <- coef(lm(bill_length_mm ~ species, \n                           data = penguins_with_contrasts)) \n\n# I'm using list() to print and caption results side by side, purely aesthetic\nlist(\"(Default) Treatment Coding\" = treatment_coefs,\n     \"(Manual) Scaled Sum Coding\" = scaledsum_coefs)\n\n\n$`(Default) Treatment Coding`\n     (Intercept) speciesChinstrap    speciesGentoo \n       38.791391        10.042433         8.713487 \n\n$`(Manual) Scaled Sum Coding`\n(Intercept)    species1    species2 \n  45.043364   10.042433    8.713487 \n\n\nThe model coefficients for the scaled sum coding shows the same pairwise comparisons as the model using treatment coding, but the intercepts differ. We can check what they correspond to manually:\n\n\nCode\ngroup_means <- \n  penguins |>\n  dplyr::group_by(species) |> \n  dplyr::summarize(mean_length = mean(bill_length_mm, na.rm = TRUE)) |> \n  purrr::pluck('mean_length') |> \n  `names<-`(c('Adelie', 'Chinstrap', 'Gentoo'))\n\nlist(\"Group means\"= group_means,\n     \"Grand mean\" = mean(group_means))\n\n\n$`Group means`\n   Adelie Chinstrap    Gentoo \n 38.79139  48.83382  47.50488 \n\n$`Grand mean`\n[1] 45.04336\n\n\nSo the intercept for the treatment coded model is the mean of the Adelie group while the scaled sum coded model is the grand mean, or the mean of group means. But, typing in the scaled sum contrast matrix was a bit obnoxious with all the -1/3 we typed. If we had made a slight mistake while typing the matrix out, what would have happened to our model? Would our coefficients reflect the averages and differences we were expecting? As an example, let’s see what happens when we change a 2/3 to 1/3:\n\n\nCode\n# What if we accidentally typed 1/3 instead of 2/3?\ncontrasts(penguins_with_contrasts$species) <- matrix(c(-1/3, 1/3, -1/3,\n                                                       -1/3, -1/3, 2/3),\n                                                     nrow = 3)\nmistake_coefs <- coef(lm(bill_length_mm ~ species, \n                         data = penguins_with_contrasts))\n\nlist(\"(Current) Mistaken Scaled Sum Coding:\" = mistake_coefs,\n     \"(Previous) Correct Scaled Sum Coding:\" = scaledsum_coefs)\n\n\n$`(Current) Mistaken Scaled Sum Coding:`\n(Intercept)    species1    species2 \n  46.717103   15.063649    8.713487 \n\n$`(Previous) Correct Scaled Sum Coding:`\n(Intercept)    species1    species2 \n  45.043364   10.042433    8.713487 \n\n\nHere we can see that the intercept and the value for species1 have increased in magnitude. In particular, the new reported effect of species1 is much larger than it previously was. If we stopped at this point, we would conclude that the difference in bill length between the Chinstrap and Adelie groups is a whopping 15mm (remember we originally calculated it to be about 10). If we were interested in whether there was a positive or negative difference that was significant or not, we’d still make that conclusion, but any claims about the magnitude of the effect would be misguided. This problem opens up a related question though: What does this new inflated-in-magitude coefficient estimate represent?\n\nDiagnosing our mistake\nTo check what these numbers correspond to, we have to check the hypothesis matrix that corresponds to our contrast matrix. The process of obtaining the hypothesis matrix has been referred to as finding the generalized inverse of the contrast matrix (see Schad et al. 2020 for details).\n\n\nCode\nmatrix(c(1, 1, 1,         # Add a column of 1s for the intercept\n         -1/3, 1/3, -1/3,\n         -1/3, -1/3, 2/3),\n       nrow = 3,\n       dimnames = list(NULL, c('Intercept', 'species1', 'species2'))) |> \n  t() |> \n  solve() |> \n  MASS::fractions() # This function just shows numbers as fractions\n\n\n     Intercept species1 species2\n[1,]  1/6      -3/2       -1    \n[2,]  1/2       3/2        0    \n[3,]  1/3         0        1    \n\n\nHere the intercept is represented by the weighted sum of each group mean, where the weights are shown in the intercept column. In most cases, the intercept should reflect the grand mean, or the mean of the group means, and so would usually have equal weights (i.e., 1/3 here) for the levels. In this case, we see the fractional weights are not the same. We can verify this by calculating the weighted mean ourselves:\n\n\nCode\nlist(\"Grand Mean\" = mean(group_means),\n     \"Weighted mean\" = weighted.mean(group_means, c(1/6, 1/2, 1/3)))\n\n\n$`Grand Mean`\n[1] 45.04336\n\n$`Weighted mean`\n[1] 46.7171\n\n\nSimilarly, the coefficient for species1 shows the difference between the group means of levels 1 and 2 (i.e., mean of Chinstrap - mean of Adelie) but times a factor of 3/2. Crucially, if our goal is to evaluate the difference between the means of these two levels, then our mistake in coding the hypothesis matrix will give us a larger estimate (~15 vs 10). Consider a similar setup where the larger estimate was 5 instead of 0; if we were relying on null hypothesis testing it’s possible we’d get a significant effect when really we shouldn’t have.\n\n\nCode\nlist(\"Mistaken Scaled Sum Coding\" = mistake_coefs,\n     \"Correct Scaled Sum Coding\" = scaledsum_coefs,\n     \"Computed Chinstrap-Adelie Difference with 3/2 scaling\" = \n       (3/2 * group_means[['Chinstrap']]) - (3/2 * group_means[['Adelie']]),\n     \"Actual Chinstrap-Adelie Difference\" = \n       group_means[['Chinstrap']] - group_means[['Adelie']])\n\n\n$`Mistaken Scaled Sum Coding`\n(Intercept)    species1    species2 \n  46.717103   15.063649    8.713487 \n\n$`Correct Scaled Sum Coding`\n(Intercept)    species1    species2 \n  45.043364   10.042433    8.713487 \n\n$`Computed Chinstrap-Adelie Difference with 3/2 scaling`\n[1] 15.06365\n\n$`Actual Chinstrap-Adelie Difference`\n[1] 10.04243\n\n\nPoint being: we made an honest mistake of typing 1/3 instead of 2/3 but this had ramifications for the coefficients in our model output that we use to make inferences. In practice, because we did the multiple contrasts<- calls, we would likely assume that what we did was correct in the absence of any errors."
  },
  {
    "objectID": "posts/contrastable/index.html#tidy-approach-to-contrasts",
    "href": "posts/contrastable/index.html#tidy-approach-to-contrasts",
    "title": "Contrastable",
    "section": "Tidy approach to contrasts",
    "text": "Tidy approach to contrasts\nHere I’ll show a different approach using the contrastable package. This package takes a tidy approach to take care of the overhead of labels and reference levels involved when using common contrast coding schemes. Specifically, this package provides a series of functions that use a special formula implementation that assigns specific meanings to each operator. The left hand side of the formula is the factor column whose contrasts you want to change. The right hand side consists of (at minimum) a function to generate contrast matrices such as contr.treatment or treatment_code. Additional operators provide extra optional functionality:\n\n+ x: Set reference level to level x\n* x: Set intercept to be the mean of x\n- 3:4: For polynomial contrasts only, drop trends 3 and 4\n| c(\"A-B\", \"A-C\"): Set the comparison labels to A-B and A-C (must be the last operator if used)\n\nRecall that in many cases researchers want pairwise comparisons while retaining main effects, and so the choice of reference level for the comparisons is very important. By default, R uses the first level alphabetically as the reference level, but sometimes we want to change this manually ourselves. Here’s an example where we set the sex and species factors to the two contrast schemes we manually set before. The set_contrasts function will show a message if it detects additional factor variables in the dataframe that the user did not provide contrasts for.\n\n\nCode\n# library(contrastable) was loaded earlier\npenguins_df <- \n  penguins |> \n  set_contrasts(sex ~ scaled_sum_code + \"male\", # Set reference level with +\n                species ~ scaled_sum_code + 'Adelie') \n\n\nExpect contr.treatment or contr.poly for unset factors: island\n\n\nCode\ncontrasts(penguins_df$species) |> MASS::fractions()\n\n\n          Chinstrap Gentoo\nAdelie    -1/3      -1/3  \nChinstrap  2/3      -1/3  \nGentoo    -1/3       2/3  \n\n\nCode\ncontrasts(penguins_df$sex) |> MASS::fractions()\n\n\n       female\nfemale  1/2  \nmale   -1/2  \n\n\npenguins_df now has its contrasts set, and we can run our model as usual. Note that we didn’t have to type out any matrices ourselves, but we got the correct contrasts that we needed.\n\n\nCode\ncoef(lm(bill_length_mm ~ species + bill_depth_mm, data = penguins_df))\n\n\n     (Intercept) speciesChinstrap    speciesGentoo    bill_depth_mm \n       20.997115         9.938955        13.403279         1.394011 \n\n\nIf we wanted to change the labels to better reflect the comparisons being made, we could do that in the formula too with the | operator.\n\n\nCode\npenguins_df <- \n  penguins_df |> \n  set_contrasts(species ~ scaled_sum_code + 'Adelie' | \n                  c('Chinstrap-Ad', 'Gentoo-Ad'))\n\ncoef(lm(bill_length_mm ~ species, data = penguins_df))\n\n\n        (Intercept) speciesChinstrap-Ad    speciesGentoo-Ad \n          45.043364           10.042433            8.713487 \n\n\n\nAdditional functions\nTypically when I use this package in my analyses the set_contrasts function is all I really need, but there are other functions that follow the same syntax that provide other information. To avoid retyping things, I’ll usually keep the contrasts in a list assigned to a separate variable and pass that to functions.\nThe glimpse_contrasts function can show information about the factors in a dataset along with the contrast schemes that have been assigned to each factor.\n\n\nCode\nmy_contrasts <- \n  list(\n    sex ~ scaled_sum_code + 'female',\n    species ~ helmert_code\n  )\n\nglimpse_contrasts(penguins_df, my_contrasts) |> gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      factor\n      n_levels\n      level_names\n      scheme\n      reference\n      intercept\n      orthogonal\n      centered\n      dropped_trends\n      explicitly_set\n    \n  \n  \n    sex\n2\nfemale, male\nscaled_sum_code\nfemale\ngrand mean\nNA\nTRUE\nNA\nTRUE\n    species\n3\nAdelie, Chinstrap, Gentoo\nhelmert_code\nGentoo\ngrand mean\nTRUE\nTRUE\nNA\nTRUE\n    island\n3\nBiscoe, Dream, Torgersen\ncontr.treatment\nBiscoe\nmean(Biscoe)\nFALSE\nFALSE\nNA\nFALSE\n  \n  \n  \n\n\n\n\nThe enlist_contrasts function does the same thing as set_contrasts, but returns a list of contrast matrices that can be used in the contrasts argument of some model-fitting functions.4 It also provides an easy way to show the contrast matrices in an appendix or supplementary material.\n\n\nCode\nenlist_contrasts(penguins_df, my_contrasts) |> purrr::map(MASS::fractions)\n\n\nExpect contr.treatment or contr.poly for unset factors: island\n\n\n$sex\n       male\nfemale -1/2\nmale    1/2\n\n$species\n          >Adelie >Chinstrap\nAdelie     2/3       0      \nChinstrap -1/3     1/2      \nGentoo    -1/3    -1/2      \n\n\n\n\nAvailable contrast schemes\nHere are the different contrast functions this package currently provides.\n\nTreatmentSumScaled SumHelmertReverse HelmertForward DifferenceBackward DifferenceOrthogonal PolynomialsRaw Polynomials\n\n\n\n\nCode\n# = contr.treatment\ntreatment_code(5) |> MASS::fractions()\n\n\n  2 3 4 5\n1 0 0 0 0\n2 1 0 0 0\n3 0 1 0 0\n4 0 0 1 0\n5 0 0 0 1\n\n\n\n\n\n\nCode\n# = contr.sum\nsum_code(5) |> MASS::fractions()\n\n\n  [,1] [,2] [,3] [,4]\n1  1    0    0    0  \n2  0    1    0    0  \n3  0    0    1    0  \n4  0    0    0    1  \n5 -1   -1   -1   -1  \n\n\n\n\n\n\nCode\n# = contr.sum\nscaled_sum_code(5) |> MASS::fractions()\n\n\n  [,1] [,2] [,3] [,4]\n1  4/5 -1/5 -1/5 -1/5\n2 -1/5  4/5 -1/5 -1/5\n3 -1/5 -1/5  4/5 -1/5\n4 -1/5 -1/5 -1/5  4/5\n5 -1/5 -1/5 -1/5 -1/5\n\n\n\n\n\n\nCode\n# NOT = contr.helmert, which is unscaled\nhelmert_code(5) |> MASS::fractions()\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  4/5    0    0    0\n[2,] -1/5  3/4    0    0\n[3,] -1/5 -1/4  2/3    0\n[4,] -1/5 -1/4 -1/3  1/2\n[5,] -1/5 -1/4 -1/3 -1/2\n\n\n\n\n\n\nCode\nreverse_helmert_code(5) |> MASS::fractions()\n\n\n     [,1] [,2] [,3] [,4]\n[1,] -1/2 -1/3 -1/4 -1/5\n[2,]  1/2 -1/3 -1/4 -1/5\n[3,]    0  2/3 -1/4 -1/5\n[4,]    0    0  3/4 -1/5\n[5,]    0    0    0  4/5\n\n\n\n\n\n\nCode\nforward_difference_code(5) |> MASS::fractions()\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  4/5  3/5  2/5  1/5\n[2,] -1/5  3/5  2/5  1/5\n[3,] -1/5 -2/5  2/5  1/5\n[4,] -1/5 -2/5 -3/5  1/5\n[5,] -1/5 -2/5 -3/5 -4/5\n\n\n\n\n\n\nCode\nbackward_difference_code(5) |> MASS::fractions()\n\n\n     [,1] [,2] [,3] [,4]\n[1,] -4/5 -3/5 -2/5 -1/5\n[2,]  1/5 -3/5 -2/5 -1/5\n[3,]  1/5  2/5 -2/5 -1/5\n[4,]  1/5  2/5  3/5 -1/5\n[5,]  1/5  2/5  3/5  4/5\n\n\n\n\n\n\nCode\n# = contr.poly, poly(1:n, degree = n-1, raw = FALSE)\north_polynomial_code(5) |> MASS::fractions()\n\n\n     .L                .Q                .C                ^4               \n[1,]          -265/419          929/1738  -2026009/6406803        4018/33617\n[2,]  -2026009/6406803         -809/3027           191/302    -246481/515552\n[3,]                 0       -6263/11717                 0         1042/1453\n[4,] 12484830/39480499         -809/3027          -265/419    -246481/515552\n[5,]           191/302          929/1738 10458821/33073696        4018/33617\n\n\n\n\n\n\nCode\n# = poly(1:n, degree = n-1, raw = TRUE)\nraw_polynomial_code(5) |> MASS::fractions()\n\n\n     1   2   3   4  \n[1,]   1   1   1   1\n[2,]   2   4   8  16\n[3,]   3   9  27  81\n[4,]   4  16  64 256\n[5,]   5  25 125 625"
  },
  {
    "objectID": "posts/contrastable/index.html#other-packages-and-resources",
    "href": "posts/contrastable/index.html#other-packages-and-resources",
    "title": "Contrastable",
    "section": "Other packages and resources",
    "text": "Other packages and resources\nThis package is not the first package made for contrast coding, though to my knowledge it is the first to take a “tidy” approach to it.\nThe hypr package (Rabe et al. 2020) takes a different approach, where the focus is on considering the hypothesis matrix and declaring specifically which comparisons you want to make, then the package can provide a corresponding matrix. I like hypr a lot actually, but I find it a bit tedious when I know what the contrast matrix should look like but I have to type out the comparisons; still better than matrix calls though.\nThe emmeans package (Lenth 2022) is extremely useful for making pairwise comparisons, but is capable of a lot more as well. You can see its vignette on contrasts here.\nThe multcomp package (Hothorn, Bretz, and Westfall 2008) is useful for simultaneous inference, which seeks to extend workflows for multiple comparisons.\nI haven’t used the contrasts package (O’Callaghan 2021) very much, but judging from its vignette here it seems like it extends the rms package’s contrast function (Harrell Jr 2022). It seems useful for calculating different comparisons after a model is run, but its usage isn’t very transparent to me on first glance.\nWhile not a package, this page from UCLA pops up a lot when people discuss contrast coding. It’s very useful, and I used it as a starting point for implementing different contrast functions. However, I will note that I don’t follow its naming conventions.\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] palmerpenguins_0.1.1 purrr_1.0.0          dplyr_1.1.0         \n[4] contrastable_0.1.0  \n\nloaded via a namespace (and not attached):\n [1] pillar_1.8.1      compiler_4.2.2    tools_4.2.2       digest_0.6.31    \n [5] jsonlite_1.8.4    evaluate_0.20     lifecycle_1.0.3   tibble_3.1.8     \n [9] gtable_0.3.1      pkgconfig_2.0.3   rlang_1.0.6       cli_3.5.0        \n[13] rstudioapi_0.14   yaml_2.3.7        xfun_0.37         fastmap_1.1.0    \n[17] withr_2.5.0       stringr_1.5.0     knitr_1.42        sass_0.4.5       \n[21] generics_0.1.3    vctrs_0.5.2       htmlwidgets_1.6.1 grid_4.2.2       \n[25] tidyselect_1.2.0  glue_1.6.2        R6_2.5.1          fansi_1.0.3      \n[29] rmarkdown_2.20    ggplot2_3.4.0     magrittr_2.0.3    scales_1.2.1     \n[33] htmltools_0.5.4   MASS_7.3-58.1     gt_0.8.0          colorspace_2.0-3 \n[37] utf8_1.2.2        stringi_1.7.8     munsell_0.5.0     crayon_1.5.2"
  },
  {
    "objectID": "posts/pulselabeling/pulse_labeling.html",
    "href": "posts/pulselabeling/pulse_labeling.html",
    "title": "Pulse Labeling",
    "section": "",
    "text": "A colleague of mine recently asked for help with a bit of a tricky problem. Pitch contours, when extracted from an acoustic signal, come in the form of a time series of discretized pulses: a (time, frequency) point. However, these contours occur over varying parts of the spoken sentence. In other words, the rises and falls in pitch occur over different words, syllables, and phones– all of which have varying durations. If we take a pitch contour over a single word from the productions of many speakers, then time normalize them, we don’t have access to the relative durations and landmarks of the syllables that make up the token. For instance, if two speakers rise from 110Hz to 220Hz over one syllable, this rise will be much steeper if the first syllable is 30% of the time normalized duration compared to if the first syllable is 60% of the time normalized duration. The problem then becomes two related problems:\nAs it happens, this problem becomes fairly straightforward when using non-equi joins. These are joins (left, right, etc.) that match not merely on equivalence matching, but when more complex logical operations need to be used. In our case, we want to merge information about the pulses and their timestamps with information about the syllable boundary timestamps. We can accomplish this by joining two dataframes based on whether the pulse timestamp is between the start and end timestamps of a particular interval.\nIn this post, I’ll show how this can be accomplished in R using the new non-equi join functionality added to {dplyr} version 1.1.1 In the process, I’ll also show off some tools from the {rPraat} package and the {sosprosody} package, which I am developing to support the analyses and data processing for my dissertation work. These packages provide some useful functionality for working with PitchTier and TextGrid objects from Praat. However, this functionality can also be accomplished through any non-equi join implementation. I’m familiar with the {fuzzyjoin} package, and this post provides some additional options."
  },
  {
    "objectID": "posts/pulselabeling/pulse_labeling.html#required-packages",
    "href": "posts/pulselabeling/pulse_labeling.html#required-packages",
    "title": "Pulse Labeling",
    "section": "Required packages",
    "text": "Required packages\nSee below code block for installation of the most relevant packages. {dplyr} must be version 1.1/the development version (as of this writing) to allow for non-equi/fuzzy joins, and so is not (yet) part of a typical installation of tidyverse.2 Refer to below code block for installation.\n\n\nCode\n# devtools or remotes can be used to install from github\ndevtools::install_github(\"tidyverse/dplyr\")\ninstall.packages('rPRaat')\ndevtools::install_github('tsostarics/sosprosody')\n\n\nNow we’ll load the packages we’ll be using.\n\n\nCode\nlibrary(dplyr)      # For joins\nlibrary(rPraat)     # Read functions for textgrids and pitch tiers\nlibrary(sosprosody) # Helpers for working with textgrids and pitch tiers\n\n# These two are part of any tidyverse installation\nlibrary(ggplot2)    # For plotting\nlibrary(purrr)      # For mapping functions"
  },
  {
    "objectID": "posts/pulselabeling/pulse_labeling.html#piecewise-equal-pulses",
    "href": "posts/pulselabeling/pulse_labeling.html#piecewise-equal-pulses",
    "title": "Pulse Labeling",
    "section": "Piecewise equal pulses",
    "text": "Piecewise equal pulses\nThis is an example using the piecewise_interpolate_pulses function3 from sosprosody to get equally spaced pulses within each section.4 Here I’ll get 50 equally spaced pulses for each syllable in all of our words (which are all two syllables).\n\n\nCode\nsyllable_labeled_df <- \n  label_pitch_pulses(tier = 'syllable') |> \n  dplyr::filter(interval_i != 1) |> \n  rename(syllable = label)\n\nsyllable_labeled_df |> \n  # Get fifty equally spaced pulses for each interval\n  sosprosody::piecewise_interpolate_pulses(section_by = \"syllable\",\n                                           pulses_per_section = 50,\n                                           time_by = \"timepoint\",\n                                           .pitchval = \"hz\",\n                                           .grouping = \"file\") |> \n  ggplot(aes(x = timepoint, y = hz, color= syllable, shape = syllable)) +\n  geom_line() +\n  geom_point(size = 2) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top') +\n  facet_wrap(~file) +\n  coord_fixed(1/50)\n\n\n\n\n\nMoreover, piecewise_interpolate_pulses can be used to get a certain number of pulses per section. This allows you to upsample or downsample different parts of the utterance as needed (cf the above example upsampled across the board). Below, I’ll get 10 pulses for the first section and 40 pulses for the second.5\n\n\nCode\nsyllable_labeled_df |> \n  # Get different numbers of pulses by interval\n  sosprosody::piecewise_interpolate_pulses(section_by = \"syllable\",\n                                           pulses_per_section = c('1' = 10,\n                                                                  '2' = 40),\n                                           time_by = \"timepoint\",\n                                           .grouping = \"file\") |> \n  ggplot(aes(x = timepoint,\n             y = hz,\n             group = file,\n             color = syllable, \n             shape = syllable)) +\n  geom_line() +\n  geom_point(size = 2) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top')\n\n\n\n\n\nIn the above plot, we can tell that at least one of the files have a pitch contour that starts a bit later than the others given the raw time on the x axis. This arises due to the slightly longer prenuclear material present in that utterance (the other two files were resynthesized from the same source file). We can time normalize these files such that the first syllable from each contour is directly comparable, i.e., they’ll all start at 0 and end at 1. And we can do that separately for the second syllable too. It’s more typical to time normalize by the duration of an entire word or utterance, but we’ll work with a more narrow case of time normalizing within each syllable. We address this problem with with much of the tools we’ve already used so far. Below I use the time_normalize function from sosprosody, which is a convenience wrapper that does the subtraction and division operations for time normalizing.6\n\n\nCode\n# Time normalize by syllable\nsyllable_labeled_df |> \n  group_by(file, interval_i) |> \n  sosprosody::time_normalize(.to = 'time_norm') |> \n  sosprosody::piecewise_interpolate_pulses(section_by = \"syllable\",\n                                           pulses_per_section = c('1' = 10,\n                                                                  '2' = 40),\n                                           time_by = \"time_norm\",\n                                           .grouping = \"file\") |> \n  ggplot(aes(x = time_norm,\n             y = hz,\n             group = file,\n             color = syllable, \n             shape = syllable)) +\n  geom_line() +\n  geom_point(size = 2) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top') +\n  facet_wrap(~syllable) +\n  coord_fixed(1/50)\n\n\n\n\n\nAnd that’s all! If you’d like to see a bigger example of how it might be useful to label regions of a pitch contour or extract equally spaced samples, you can take a look at the plots on this poster. I’ve used this approach to extract less pulses from the prenuclear region of the utterance and more pulses from the nuclear region, then average multiple pitch contours across the extracted pulses.\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] purrr_1.0.0           ggplot2_3.4.0         sosprosody_0.0.0.9000\n[4] rPraat_1.3.2-1        dplyr_1.1.0          \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9        pillar_1.8.1      compiler_4.2.2    tools_4.2.2      \n [5] bit_4.0.5         digest_0.6.31     jsonlite_1.8.4    evaluate_0.20    \n [9] lifecycle_1.0.3   tibble_3.1.8      gtable_0.3.1      pkgconfig_2.0.3  \n[13] rlang_1.0.6       cli_3.5.0         rstudioapi_0.14   parallel_4.2.2   \n[17] yaml_2.3.7        xfun_0.37         fastmap_1.1.0     withr_2.5.0      \n[21] knitr_1.42        generics_0.1.3    vctrs_0.5.2       htmlwidgets_1.6.1\n[25] hms_1.1.2         bit64_4.0.5       grid_4.2.2        tidyselect_1.2.0 \n[29] glue_1.6.2        R6_2.5.1          fansi_1.0.3       vroom_1.6.1      \n[33] rmarkdown_2.20    farver_2.1.1      tzdb_0.3.0        readr_2.1.3      \n[37] magrittr_2.0.3    ellipsis_0.3.2    scales_1.2.1      htmltools_0.5.4  \n[41] colorspace_2.0-3  labeling_0.4.2    utf8_1.2.2        munsell_0.5.0    \n[45] crayon_1.5.2"
  },
  {
    "objectID": "posts/pulselabeling/pulse_labeling.html#example-with-one-file",
    "href": "posts/pulselabeling/pulse_labeling.html#example-with-one-file",
    "title": "Pulse Labeling",
    "section": "Example with one file",
    "text": "Example with one file\nOur goal: Associate each pitch pulse with an interval on a tier from a TextGrid.\nThe approach: Non-equi joins. Given a dataframe for the pitch pulses and a dataframe for a tier from a TextGrid, when a pitch pulse timepoint lies between interval_start and interval_end, join interval_start, interval_end, and the interval label.\nFirst we’ll load our files, located in the Files directory. Because I’m working with just one example right now, I’ll write the strings out directly.\n\n\nCode\npt_file <- \"Files/branning_1.PitchTier\"\ntg_file <- \"Files/branning_1.TextGrid\"\n\n\nNow we’ll read the PitchTier and TextGrid files into R using rPraat. The sosprosody package provides new print methods for PitchTier and TextGrid objects, which can also be seen below (these would be displayed in the console).\n\n\nCode\n# The default encoding is UTF-8, but there's a common \n# case where Praat encodes files with UTF-16LE\npt <- rPraat::pt.read(pt_file, encoding= \"auto\")\ntg <- rPraat::tg.read(tg_file, encoding = \"auto\")\n\n\n\n\n\n\nbranning_1.PitchTier: 108 total pitch pulses.\n109|                                                                           |\n   |                                             OOO                           |\n   |                                          OOO   OO                         |\n   |                                      OOOO        OO                       |\n   |                                  OOOO              O                      |\n   |                                OO                   OO                    |\n   |      OOOOOOOOOOOOOOOOOOOOOOOOOO                       OOO                 |\n   |                                                          OOO              |\n   |                                                             OOOO          |\n 65|                                                                 OOO       |\n   0                                                                        1.49 \n\n\nbranning_1.TextGrid\n[                words: 4/6 labeled intervals from 0 to 1.49                   ]\n[                phones: 15/17 labeled intervals from 0 to 1.49                ]\n[                nuclear: 1/3 labeled intervals from 0 to 1.49                 ]\n[                syllable: 2/4 labeled intervals from 0 to 1.49                ] \n\n\n\n\nNow that we have access to our PitchTier and TextGrid, we can convert them into data frame representations, again using sosprosody. We’ll limit our scope right now to just the phones tier.\n\n\nCode\n# TextGrid as a a dataframe\ntiers <- sosprosody::textgrid_to_dataframes(tg) # List of dfs for each tier\ntier_df <- tiers$phones # Extract a single tier\n\n# Small post processing steps\ntier_df$file <- gsub(\".TextGrid\", \"\", tier_df$file, perl = TRUE)\ntier_df$interval_i <- seq_len(nrow(tier_df)) # Assign numeric indices\nhead(tier_df)\n\n\n        file interval_start interval_end label interval_i\n1 branning_1      0.0000000    0.1312844                1\n2 branning_1      0.1312844    0.2200000     m          2\n3 branning_1      0.2200000    0.2500000     ɑ          3\n4 branning_1      0.2500000    0.3800000     ʎ          4\n5 branning_1      0.3800000    0.4297910     i          5\n6 branning_1      0.4297910    0.4910510     z          6\n\n\nCode\n# Pitchtier as a dataframe, we don't need to calculate semitones and ERBs\npt_df <- sosprosody::pitchtier_to_dataframe(pt, \n                                            add_semitones = FALSE,\n                                            add_erbs = FALSE)\nhead(pt_df)\n\n\n        file timepoint       hz\n1 branning_1 0.1514512 89.57813\n2 branning_1 0.1614512 89.96063\n3 branning_1 0.1714512 89.98904\n4 branning_1 0.1814512 89.94582\n5 branning_1 0.1914512 89.89566\n6 branning_1 0.2014512 89.87433\n\n\nNext we’re going to use the new non-equi join functionality from dplyr. We want to match up the file in tier_df with the correct file in pt_df, then check for when the pulse timepoint (i.e., pt_df$timepoint) lies after interval_start and before interval_end. The choice of >= and < is largely arbitrary, you could do > and <= and get the same results unless you have pitch points that lie exactly on the boundary.\n\n\nCode\nlabeled_df <- \n  dplyr::left_join(pt_df, \n                   tier_df, \n                   join_by(file, \n                           timepoint >= interval_start, \n                           timepoint < interval_end))\n\nhead(labeled_df)\n\n\n        file timepoint       hz interval_start interval_end label interval_i\n1 branning_1 0.1514512 89.57813      0.1312844         0.22     m          2\n2 branning_1 0.1614512 89.96063      0.1312844         0.22     m          2\n3 branning_1 0.1714512 89.98904      0.1312844         0.22     m          2\n4 branning_1 0.1814512 89.94582      0.1312844         0.22     m          2\n5 branning_1 0.1914512 89.89566      0.1312844         0.22     m          2\n6 branning_1 0.2014512 89.87433      0.1312844         0.22     m          2\n\n\nTo check our work, let’s plot the pitch track to see what our result is like. Remember, we wanted to associate each pulse with the interval it appears in.\n\n\nCode\nlabeled_df |> \n  ggplot(aes(x = timepoint, y = hz, color = label, group = interval_i)) +\n  geom_line() +\n  geom_point() +\n  theme_bw(base_size = 14) +\n  # Add labels since there's a lot of colors\n  geom_label(data = summarize(group_by(labeled_df, label, interval_i),\n                              timepoint = median(timepoint),\n                              hz = median(hz) - 3),\n             aes(label = label)) +\n  theme(legend.position = 'none')\n\n\n\n\n\nLooks great! Now we can adapt this same basic workflow of loading our Praat objects, converting to dataframes, and doing a non-equi join and apply it to all the files in a directory. Note that if you extracted F0 measurements using a method other than exporting PitchTier files from Praat, then you can still do all the same steps above but just omit the part where you read in the PitchTier files. Ultimately, all we’re looking for is a dataframe that says what file(s) we have, what the pulse timepoints are, and what the frequency values at those timepoints are; how we get there doesn’t really matter."
  },
  {
    "objectID": "posts/pulselabeling/pulse_labeling.html#example-with-multiple-files",
    "href": "posts/pulselabeling/pulse_labeling.html#example-with-multiple-files",
    "title": "Pulse Labeling",
    "section": "Example with multiple files",
    "text": "Example with multiple files\nHere’s a pretty basic wrapper that encapsulates all of the previous steps into one function and operates over all the files in given directories containing TextGrids and PitchTiers.\n\n\n\n\n\n\nTip\n\n\n\nIf I were to do this for a larger set of files, I would probably split this up into two parts where I first load all the files then join them together. The reason being that if there’s an issue with joining the files in the below function, I would have to reload all the files again just to try the join again. Right now I’m only working with three files, so it doesn’t matter too much.\n\n\n\n\nCode\nlabel_pitch_pulses <- function(textgrid_dir = \"Files\", \n                               pitchtier_dir = \"Files\", \n                               tier = 'phones') {\n  # Note: This will fail if each textgrid does not have a corresponding\n  #       pitch tier file in the pitchtier directory\n  textgrids <- list.files(textgrid_dir, \n                          pattern = \".TextGrid$\",\n                          full.names = TRUE)\n  pitchtiers <- gsub(\".TextGrid$\", \".PitchTier\", textgrids, perl = TRUE)\n  \n  tg_dfs <- \n    map_dfr(textgrids,\n            \\(tg_path) {\n              tg <- rPraat::tg.read(tg_path, encoding = \"auto\")\n              tiers <- sosprosody::textgrid_to_dataframes(tg)\n              tier_df <- tiers[[tier]] \n              \n              # Small post processing steps\n              tier_df$file <- gsub(\".TextGrid\", \"\", tier_df$file, perl = TRUE)\n              tier_df$interval_i <- seq_len(nrow(tier_df))\n              \n              tier_df\n            })\n  \n  pt_dfs <- \n    map_dfr(pitchtiers,\n            \\(pt_path) {\n              pt <- rPraat::pt.read(pt_path, encoding= \"auto\")\n              pt_df <- \n                sosprosody::pitchtier_to_dataframe(pt, \n                                                   add_semitones = FALSE,\n                                                   add_erbs = FALSE)\n              \n              pt_df\n            })\n  \n  # Execute the join\n  left_join(pt_dfs, \n            tg_dfs, \n            join_by(file, \n                    timepoint >= interval_start, \n                    timepoint < interval_end))\n  \n}\n\n\nAnd now the helper can be used to process all of our files; we’ll use it again later when we start working with syllables instead of phones.\n\n\nCode\nphone_labeled_df <- label_pitch_pulses()\n\n\nAs before, we can plot the pitch contours for each of our files to check our work.\n\n\nCode\nphone_labeled_df |> \n  ggplot(aes(x = timepoint, y = hz, group = interval_i, color = label)) +\n  geom_line() +\n  geom_point(size = .5) +\n  facet_wrap(~file) +\n  theme_bw(base_size = 14) +\n  geom_text(data = summarize(group_by(phone_labeled_df, file, label, interval_i),\n                             timepoint = median(timepoint),\n                             hz = median(hz) - 4),\n            aes(label = label)) +\n  theme(legend.position = 'none') +\n  coord_fixed(1/50)\n\n\n\n\n\nNext I’ll show an example of how this might be useful beyond just labeling and coloring our pitch contours by interval."
  },
  {
    "objectID": "posts/pulselabeling/index.html",
    "href": "posts/pulselabeling/index.html",
    "title": "Pulse Labeling",
    "section": "",
    "text": "A colleague of mine recently asked for help with a bit of a tricky problem. Pitch contours, when extracted from an acoustic signal, come in the form of a time series of discretized pulses: a (time, frequency) point. However, these contours occur over varying parts of the spoken sentence. In other words, the rises and falls in pitch occur over different words, syllables, and phones– all of which have varying durations. If we take a pitch contour over a single word from the productions of many speakers, then time normalize them, we don’t have access to the relative durations and landmarks of the syllables that make up the token. For instance, if two speakers rise from 110Hz to 220Hz over one syllable, this rise will be much steeper if the first syllable is 30% of the time normalized duration compared to if the first syllable is 60% of the time normalized duration. The problem then becomes two related problems:\nAs it happens, this problem becomes fairly straightforward when using non-equi joins. These are joins (left, right, etc.) that match not merely on equivalence matching, but when more complex logical operations need to be used. In our case, we want to merge information about the pulses and their timestamps with information about the syllable boundary timestamps. We can accomplish this by joining two dataframes based on whether the pulse timestamp is between the start and end timestamps of a particular interval.\nIn this post, I’ll show how this can be accomplished in R using the new non-equi join functionality added to {dplyr} version 1.1.1 In the process, I’ll also show off some tools from the {rPraat} package and the {sosprosody} package, which I am developing to support the analyses and data processing for my dissertation work. These packages provide some useful functionality for working with PitchTier and TextGrid objects from Praat. However, this functionality can also be accomplished through any non-equi join implementation. I’m familiar with the {fuzzyjoin} package, and this post provides some additional options."
  },
  {
    "objectID": "posts/pulselabeling/index.html#required-packages",
    "href": "posts/pulselabeling/index.html#required-packages",
    "title": "Pulse Labeling",
    "section": "Required packages",
    "text": "Required packages\nSee below code block for installation of the most relevant packages. {dplyr} must be version 1.1/the development version (as of this writing) to allow for non-equi/fuzzy joins, and so is not (yet) part of a typical installation of tidyverse.2 Refer to below code block for installation.\n\n\nCode\n# devtools or remotes can be used to install from github\ndevtools::install_github(\"tidyverse/dplyr\")\ninstall.packages('rPRaat')\ndevtools::install_github('tsostarics/sosprosody')\n\n\nNow we’ll load the packages we’ll be using.\n\n\nCode\nlibrary(dplyr)      # For joins\nlibrary(rPraat)     # Read functions for textgrids and pitch tiers\nlibrary(sosprosody) # Helpers for working with textgrids and pitch tiers\n\n# These two are part of any tidyverse installation\nlibrary(ggplot2)    # For plotting\nlibrary(purrr)      # For mapping functions"
  },
  {
    "objectID": "posts/pulselabeling/index.html#example-with-one-file",
    "href": "posts/pulselabeling/index.html#example-with-one-file",
    "title": "Pulse Labeling",
    "section": "Example with one file",
    "text": "Example with one file\nOur goal: Associate each pitch pulse with an interval on a tier from a TextGrid.\nThe approach: Non-equi joins. Given a dataframe for the pitch pulses and a dataframe for a tier from a TextGrid, when a pitch pulse timepoint lies between interval_start and interval_end, join interval_start, interval_end, and the interval label.\nFirst we’ll load our files, located in the Files directory. Because I’m working with just one example right now, I’ll write the strings out directly.\n\n\nCode\npt_file <- \"Files/branning_1.PitchTier\"\ntg_file <- \"Files/branning_1.TextGrid\"\n\n\nNow we’ll read the PitchTier and TextGrid files into R using rPraat. The sosprosody package provides new print methods for PitchTier and TextGrid objects, which can also be seen below (these would be displayed in the console).\n\n\nCode\n# The default encoding is UTF-8, but there's a common \n# case where Praat encodes files with UTF-16LE\npt <- rPraat::pt.read(pt_file, encoding= \"auto\")\ntg <- rPraat::tg.read(tg_file, encoding = \"auto\")\n\n\n\n\n\n\nbranning_1.PitchTier: 108 total pitch pulses.\n109|                                                                           |\n   |                                             OOO                           |\n   |                                          OOO   OO                         |\n   |                                      OOOO        OO                       |\n   |                                  OOOO              O                      |\n   |                                OO                   OO                    |\n   |      OOOOOOOOOOOOOOOOOOOOOOOOOO                       OOO                 |\n   |                                                          OOO              |\n   |                                                             OOOO          |\n 65|                                                                 OOO       |\n   0                                                                        1.49 \n\n\nbranning_1.TextGrid\n[                words: 4/6 labeled intervals from 0 to 1.49                   ]\n[                phones: 15/17 labeled intervals from 0 to 1.49                ]\n[                nuclear: 1/3 labeled intervals from 0 to 1.49                 ]\n[                syllable: 2/4 labeled intervals from 0 to 1.49                ] \n\n\n\n\nNow that we have access to our PitchTier and TextGrid, we can convert them into data frame representations, again using sosprosody. We’ll limit our scope right now to just the phones tier.\n\n\nCode\n# TextGrid as a a dataframe\ntiers <- sosprosody::textgrid_to_dataframes(tg) # List of dfs for each tier\ntier_df <- tiers$phones # Extract a single tier\n\n# Small post processing steps\ntier_df$file <- gsub(\".TextGrid\", \"\", tier_df$file, perl = TRUE)\ntier_df$interval_i <- seq_len(nrow(tier_df)) # Assign numeric indices\nhead(tier_df)\n\n\n        file interval_start interval_end label interval_i\n1 branning_1      0.0000000    0.1312844                1\n2 branning_1      0.1312844    0.2200000     m          2\n3 branning_1      0.2200000    0.2500000     ɑ          3\n4 branning_1      0.2500000    0.3800000     ʎ          4\n5 branning_1      0.3800000    0.4297910     i          5\n6 branning_1      0.4297910    0.4910510     z          6\n\n\nCode\n# Pitchtier as a dataframe, we don't need to calculate semitones and ERBs\npt_df <- sosprosody::pitchtier_to_dataframe(pt, \n                                            add_semitones = FALSE,\n                                            add_erbs = FALSE)\nhead(pt_df)\n\n\n        file timepoint       hz\n1 branning_1 0.1514512 89.57813\n2 branning_1 0.1614512 89.96063\n3 branning_1 0.1714512 89.98904\n4 branning_1 0.1814512 89.94582\n5 branning_1 0.1914512 89.89566\n6 branning_1 0.2014512 89.87433\n\n\nNext we’re going to use the new non-equi join functionality from dplyr. We want to match up the file in tier_df with the correct file in pt_df, then check for when the pulse timepoint (i.e., pt_df$timepoint) lies after interval_start and before interval_end. The choice of >= and < is largely arbitrary, you could do > and <= and get the same results unless you have pitch points that lie exactly on the boundary.\n\n\nCode\nlabeled_df <- \n  dplyr::left_join(pt_df, \n                   tier_df, \n                   join_by(file, \n                           timepoint >= interval_start, \n                           timepoint < interval_end))\n\nhead(labeled_df)\n\n\n        file timepoint       hz interval_start interval_end label interval_i\n1 branning_1 0.1514512 89.57813      0.1312844         0.22     m          2\n2 branning_1 0.1614512 89.96063      0.1312844         0.22     m          2\n3 branning_1 0.1714512 89.98904      0.1312844         0.22     m          2\n4 branning_1 0.1814512 89.94582      0.1312844         0.22     m          2\n5 branning_1 0.1914512 89.89566      0.1312844         0.22     m          2\n6 branning_1 0.2014512 89.87433      0.1312844         0.22     m          2\n\n\nTo check our work, let’s plot the pitch track to see what our result is like. Remember, we wanted to associate each pulse with the interval it appears in.\n\n\nCode\nlabeled_df |> \n  ggplot(aes(x = timepoint, y = hz, color = label, group = interval_i)) +\n  geom_line() +\n  geom_point() +\n  theme_bw(base_size = 14) +\n  # Add labels since there's a lot of colors\n  geom_label(data = summarize(group_by(labeled_df, label, interval_i),\n                              timepoint = median(timepoint),\n                              hz = median(hz) - 3),\n             aes(label = label)) +\n  theme(legend.position = 'none')\n\n\n\n\n\nLooks great! Now we can adapt this same basic workflow of loading our Praat objects, converting to dataframes, and doing a non-equi join and apply it to all the files in a directory. Note that if you extracted F0 measurements using a method other than exporting PitchTier files from Praat, then you can still do all the same steps above but just omit the part where you read in the PitchTier files. Ultimately, all we’re looking for is a dataframe that says what file(s) we have, what the pulse timepoints are, and what the frequency values at those timepoints are; how we get there doesn’t really matter."
  },
  {
    "objectID": "posts/pulselabeling/index.html#example-with-multiple-files",
    "href": "posts/pulselabeling/index.html#example-with-multiple-files",
    "title": "Pulse Labeling",
    "section": "Example with multiple files",
    "text": "Example with multiple files\nHere’s a pretty basic wrapper that encapsulates all of the previous steps into one function and operates over all the files in given directories containing TextGrids and PitchTiers.\n\n\n\n\n\n\nTip\n\n\n\nIf I were to do this for a larger set of files, I would probably split this up into two parts where I first load all the files then join them together. The reason being that if there’s an issue with joining the files in the below function, I would have to reload all the files again just to try the join again. Right now I’m only working with three files, so it doesn’t matter too much.\n\n\n\n\nCode\nlabel_pitch_pulses <- function(textgrid_dir = \"Files\", \n                               pitchtier_dir = \"Files\", \n                               tier = 'phones') {\n  # Note: This will fail if each textgrid does not have a corresponding\n  #       pitch tier file in the pitchtier directory\n  textgrids <- list.files(textgrid_dir, \n                          pattern = \".TextGrid$\",\n                          full.names = TRUE)\n  pitchtiers <- gsub(\".TextGrid$\", \".PitchTier\", textgrids, perl = TRUE)\n  \n  tg_dfs <- \n    map_dfr(textgrids,\n            \\(tg_path) {\n              tg <- rPraat::tg.read(tg_path, encoding = \"auto\")\n              tiers <- sosprosody::textgrid_to_dataframes(tg)\n              tier_df <- tiers[[tier]] \n              \n              # Small post processing steps\n              tier_df$file <- gsub(\".TextGrid\", \"\", tier_df$file, perl = TRUE)\n              tier_df$interval_i <- seq_len(nrow(tier_df))\n              \n              tier_df\n            })\n  \n  pt_dfs <- \n    map_dfr(pitchtiers,\n            \\(pt_path) {\n              pt <- rPraat::pt.read(pt_path, encoding= \"auto\")\n              pt_df <- \n                sosprosody::pitchtier_to_dataframe(pt, \n                                                   add_semitones = FALSE,\n                                                   add_erbs = FALSE)\n              \n              pt_df\n            })\n  \n  # Execute the join\n  left_join(pt_dfs, \n            tg_dfs, \n            join_by(file, \n                    timepoint >= interval_start, \n                    timepoint < interval_end))\n  \n}\n\n\nAnd now the helper can be used to process all of our files; we’ll use it again later when we start working with syllables instead of phones.\n\n\nCode\nphone_labeled_df <- label_pitch_pulses()\n\n\nAs before, we can plot the pitch contours for each of our files to check our work.\n\n\nCode\nphone_labeled_df |> \n  ggplot(aes(x = timepoint, y = hz, group = interval_i, color = label)) +\n  geom_line() +\n  geom_point(size = .5) +\n  facet_wrap(~file) +\n  theme_bw(base_size = 14) +\n  geom_text(data = summarize(group_by(phone_labeled_df, file, label, interval_i),\n                             timepoint = median(timepoint),\n                             hz = median(hz) - 4),\n            aes(label = label)) +\n  theme(legend.position = 'none') +\n  coord_fixed(1/50)\n\n\n\n\n\nNext I’ll show an example of how this might be useful beyond just labeling and coloring our pitch contours by interval."
  },
  {
    "objectID": "posts/pulselabeling/index.html#piecewise-equal-pulses",
    "href": "posts/pulselabeling/index.html#piecewise-equal-pulses",
    "title": "Pulse Labeling",
    "section": "Piecewise equal pulses",
    "text": "Piecewise equal pulses\nThis is an example using the piecewise_interpolate_pulses function3 from sosprosody to get equally spaced pulses within each section.4 Here I’ll get 50 equally spaced pulses for each syllable in all of our words (which are all two syllables).\n\n\nCode\nsyllable_labeled_df <- \n  label_pitch_pulses(tier = 'syllable') |> \n  dplyr::filter(interval_i != 1) |> \n  rename(syllable = label)\n\nsyllable_labeled_df |> \n  # Get fifty equally spaced pulses for each interval\n  sosprosody::piecewise_interpolate_pulses(section_by = \"syllable\",\n                                           pulses_per_section = 50,\n                                           time_by = \"timepoint\",\n                                           .pitchval = \"hz\",\n                                           .grouping = \"file\") |> \n  ggplot(aes(x = timepoint, y = hz, color= syllable, shape = syllable)) +\n  geom_line() +\n  geom_point(size = 2) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top') +\n  facet_wrap(~file) +\n  coord_fixed(1/50)\n\n\n\n\n\nMoreover, piecewise_interpolate_pulses can be used to get a certain number of pulses per section. This allows you to upsample or downsample different parts of the utterance as needed (cf the above example upsampled across the board). Below, I’ll get 10 pulses for the first section and 40 pulses for the second.5\n\n\nCode\nsyllable_labeled_df |> \n  # Get different numbers of pulses by interval\n  sosprosody::piecewise_interpolate_pulses(section_by = \"syllable\",\n                                           pulses_per_section = c('1' = 10,\n                                                                  '2' = 40),\n                                           time_by = \"timepoint\",\n                                           .grouping = \"file\") |> \n  ggplot(aes(x = timepoint,\n             y = hz,\n             group = file,\n             color = syllable, \n             shape = syllable)) +\n  geom_line() +\n  geom_point(size = 2) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top')\n\n\n\n\n\nIn the above plot, we can tell that at least one of the files have a pitch contour that starts a bit later than the others given the raw time on the x axis. This arises due to the slightly longer prenuclear material present in that utterance (the other two files were resynthesized from the same source file). We can time normalize these files such that the first syllable from each contour is directly comparable, i.e., they’ll all start at 0 and end at 1. And we can do that separately for the second syllable too. It’s more typical to time normalize by the duration of an entire word or utterance, but we’ll work with a more narrow case of time normalizing within each syllable. We address this problem with with much of the tools we’ve already used so far. Below I use the time_normalize function from sosprosody, which is a convenience wrapper that does the subtraction and division operations for time normalizing.6\n\n\nCode\n# Time normalize by syllable\nsyllable_labeled_df |> \n  group_by(file) |> \n  sosprosody::time_normalize(.to = 'time_norm') |> \n  sosprosody::piecewise_interpolate_pulses(section_by = \"syllable\",\n                                           pulses_per_section = c('1' = 10,\n                                                                  '2' = 40),\n                                           index_column = 'interval_i',\n                                           time_by = \"time_norm\",\n                                           .grouping = \"file\") |> \n  ggplot(aes(x = time_norm,\n             y = hz,\n             group = file,\n             color = syllable, \n             shape = syllable)) +\n  geom_line() +\n  geom_point(size = 2) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top') +\n  facet_wrap(~syllable) +\n  coord_fixed(1/50)\n\n\n\n\n\n\nHandling overlapping pulses\nThere is one edge case that is worth mentioning: when a pitch pulse lines up exactly on the boundary between sections (i.e., intervals on a textgrid) This can especially happen when using something like the montreal forced aligner to set textgrid tier boundaries; the boundaries may be placed in steps of 10 milliseconds, which might be the same step with which pitch samples are returned. For example, say we have 20 pitch pulses from time 0 to 1, and this timespan is broken up into two sections from 0 to .5 and .5 to 1. If we want to label the pulses by which section they occur in, what do we do with a pulse that falls exactly at .5? Is it part of the first section? Or the second? Previously we addressed this in our non-equi join, associating such a point with the first section by joining on \\(min(sec_t) <= t < max(sec_t)\\). We could have associated it with the second section by joining on \\(min(sec_t) < t <= max(sec_t)\\). But if we joined on \\(min(sec_t) <= t <= max(sec_t)\\), then the time \\(t\\) at .5 would be duplicated, as there would be a point at .5 at the very end of the first section and another point at .5 at the very start of the second section.\nHere’s an example of what I mean with three sections. When all the sections are plotted together, the shared timepoints overlap.\n\n\nCode\noverlapping_df <- \n  data.frame(file = 'file1',\n             section = c(rep('a', times = 2),\n                         rep('b', times = 2),\n                         rep('c', times = 2)),\n             timestamp = c(seq(0,1,length.out = 2),\n                           seq(1, 1.4, length.out = 2),\n                           seq(1.4, 2, length.out = 2)),\n             hz = c(seq(90, 110, length.out = 2),\n                    seq(110, 70, length.out = 2),\n                    seq(70, 60, length.out = 2)))\noverlap_plot <- \n  overlapping_df |> \n  ggplot(aes(x = timestamp, y = hz, color = section, shape = section)) +\n  geom_line() +\n  geom_point(size = 3, aes(shape = section, fill = section),color ='black') +\n  scale_shape_manual(values = c(21, 22, 24)) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top')\n\noverlap_plot\noverlap_plot + \n  facet_grid(~section) + \n  theme(legend.position = 'none')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis situation causes some sticky situations when using the output of piecewise_interpolate_pulses; specifically, if you’re plotting or modeling using the resulting pulse indices. Below I interpolate 15 equally spaced points for each section, then plot by pulse index. We can see that for those points that overlapped, we get some regions that are flat, which should not be there.\n\n\nCode\noverlap_pulses <- \n  overlapping_df |> \n  piecewise_interpolate_pulses(section_by = 'section',\n                               pulses_per_section = 15,\n                               time_by = 'timestamp',\n                               .grouping = 'file')\n\noverlap_pulses |> \n  ggplot(aes(x = pulse_i, y = hz)) +\n  annotate(xmin = 14.5,\n           xmax = 16.5,\n           ymin = 108,\n           ymax = 112,\n           geom = 'rect',\n           fill = 'red',\n           alpha = .4) +\n  annotate(xmin = 29.5,\n           xmax = 31.5,\n           ymin = 68,\n           ymax = 72,\n           geom = 'rect',\n           fill = 'red',\n           alpha = .4) +\n  geom_line() +\n  geom_point(size = 3, aes(shape = section, fill = section)) +\n  scale_shape_manual(values = c(21, 22, 24)) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top') +\n  coord_fixed(1/2)\n\n\n\n\n\nI’ve provided a post-processing function that can fix this (in case it’s too much trouble to try and go back and fix the original textgrids or join operations). For each instance where two adjacent pulses share the same timestamp, you can choose to keep either the left pulse (equivalent to if we had joined using \\(min(sec_t) < t <= max(sec_t)\\)) or the right pulse (=if we had joined using \\(min(sec_t) <= t < max(sec_t)\\)).\n\n\nCode\nadjusted_df_l <- \n  overlap_pulses |> \n  drop_overlapping_pulses(keep = 'left',\n                          time_by = 'timestamp',\n                          pulse_col = 'pulse_i',\n                          .grouping = 'file')\n\nadjusted_df_r <- \n  overlap_pulses |> \n  drop_overlapping_pulses(keep = 'right',\n                          time_by = 'timestamp',\n                          pulse_col = 'pulse_i',\n                          .grouping = 'file')\n\n\nkeep_left_plot <- \n  adjusted_df_l |> \n  ggplot(aes(x = pulse_i, y = hz)) +\n  geom_line() +\n  geom_point(size = 3, aes(shape = section, fill = section)) +\n  scale_shape_manual(values = c(21, 22, 24)) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'top') +\n  coord_fixed(1/2)\n\nkeep_right_plot <- \n  adjusted_df_r |> \n  ggplot(aes(x = pulse_i, y = hz)) +\n  geom_line() +\n  geom_point(size = 3, aes(shape = section, fill = section)) +\n  scale_shape_manual(values = c(21, 22, 24)) +\n  theme_bw(base_size = 14) +\n  theme(legend.position = 'none') +\n  coord_fixed(1/2)\n\nkeep_left_plot\nkeep_right_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that we originally extracted 15 pulses for each section, but deleted one from each section that contained an overlap. As a result, the number of pulses for our three sections are not 15, 15, 15 but now 15, 14, 14 (if we kept the left pulse) or 14, 14, 15 (if we kept the right pulse). If we want to have 15 for everything in the output, then we’d need to specify pulses_per_section in piecewise_interpolate_pulses to be either c(15, 16, 16) or c(16, 16, 15) depending on which pulse we intend to keep with drop_overlapping_pulses.\nAnd that’s all! If you’d like to see a bigger example of how it might be useful to label regions of a pitch contour or extract equally spaced samples, you can take a look at the plots on this poster. I’ve used this approach to extract less pulses from the prenuclear region of the utterance and more pulses from the nuclear region, then average multiple pitch contours across the extracted pulses.\n\n\nCode\nsessionInfo()\n\n\nR version 4.2.0 (2022-04-22 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] purrr_1.0.0           ggplot2_3.4.0         sosprosody_0.0.0.9000\n[4] rPraat_1.3.2-1        dplyr_1.0.99.9000    \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10       pillar_1.8.1      compiler_4.2.0    tools_4.2.0      \n [5] bit_4.0.4         digest_0.6.31     jsonlite_1.8.4    evaluate_0.19    \n [9] lifecycle_1.0.3   tibble_3.1.8      gtable_0.3.1      pkgconfig_2.0.3  \n[13] rlang_1.0.6       cli_3.5.0         rstudioapi_0.13   parallel_4.2.0   \n[17] yaml_2.3.6        xfun_0.36         fastmap_1.1.0     withr_2.5.0      \n[21] stringr_1.5.0     knitr_1.41        hms_1.1.1         generics_0.1.3   \n[25] vctrs_0.5.2       htmlwidgets_1.5.4 bit64_4.0.5       grid_4.2.0       \n[29] tidyselect_1.2.0  glue_1.6.2        R6_2.5.1          fansi_1.0.4      \n[33] vroom_1.5.7       rmarkdown_2.14    farver_2.1.1      tzdb_0.3.0       \n[37] readr_2.1.2       magrittr_2.0.3    ellipsis_0.3.2    scales_1.2.1     \n[41] htmltools_0.5.2   colorspace_2.0-3  labeling_0.4.2    utf8_1.2.2       \n[45] stringi_1.7.8     munsell_0.5.0     crayon_1.5.2"
  },
  {
    "objectID": "posts/helmert/helmert_post.html",
    "href": "posts/helmert/helmert_post.html",
    "title": "On Helmert Coding",
    "section": "",
    "text": "I’ve had many discussions with colleagues over the past year about helmert contrasts, and I’ve decided to compile some of my notes here. This post discusses helmert coding, which is a type of contrast coding scheme where comparisons are made in a “nesting” fashion. However, there are different ways to represent these nested comparisons in a contrast matrix. Their corresponding hypothesis matrices simplify to the same statistical test, but the coefficient estimates have different magnitudes. These magnitudes differ by a scaling factor, which I will show how to derive. As a result, the statistical inference drawn will be the same whether the matrix is scaled or unscaled, but using the coefficient magnitude at face-value for something else (e.g., claims about differences in reaction time, future power analyses) will be misleading.\nI will include various exercises throughout this document that you can do yourself to better understand what’s going in."
  },
  {
    "objectID": "posts/helmert/helmert_post.html#what-is-helmert-coding",
    "href": "posts/helmert/helmert_post.html#what-is-helmert-coding",
    "title": "On Helmert Coding",
    "section": "What is Helmert Coding?",
    "text": "What is Helmert Coding?\nHelmert coding is a type of contrast coding scheme that nests levels together for some of the comparisons. This is most evident when using a factor with more than 2 levels.1\nAt an abstract level, let’s say you have four groups A, B, C, and D which allots you 3 comparisons (n-1 degrees of freedom). Helmert coding allows you to compare the means of levels A and B (\\(\\mu_B - \\mu_A\\)) for your first comparison, then the mean of level C compared to the mean of levels A and B (\\(\\mu_C - \\frac{\\mu_A +\\mu_B}{2}\\)), then finally the mean of level D compared to the mean of levels A, B, C (\\(\\mu_D - \\frac{\\mu_A +\\mu_B + \\mu_C}{3}\\)).\nTo give an example, let’s say you’re interested in comparing the duration of d, n, s, and sh word initially in words like dough, no, so, show. We have four levels, so we get three comparisons. We could pick a baseline level and compare the other two levels to it, but there’s a natural structure to these levels: they are all coronal sounds2 but three are continuants and two are sibilants3 (one compact (s) and the other diffuse (sh) ). So, we could compare the two sibilants together (sh-s) and then have another comparison between nasals and sibilants (n-sib=n-(s+sh)), then a final comparison between continuant coronals and a coronal stop. Thus, we have a comparison where two of the comparisons contain nested parts of the data. What we want to see is some statistical test for these comparisons, which themselves are just differences between means. We’ll see that in some versions of Helmert coding, this is a little off.\nHelmert coding is also useful for another reason: the comparisons are mathematically orthogonal to one another, meaning that the comparisons are independent of one another. Read more about this here and here."
  },
  {
    "objectID": "posts/helmert/helmert_post.html#issue-with-contr.helmert",
    "href": "posts/helmert/helmert_post.html#issue-with-contr.helmert",
    "title": "On Helmert Coding",
    "section": "Issue with contr.helmert",
    "text": "Issue with contr.helmert\nOne issue with the contr.helmert function provided in all installations of R via the stats package though is that the resulting coefficient estimates don’t straightforwardly encode the differences between means. Rather, the results are scaled by some multiplicative factor. We’ll build up to this with a toy example.\n\n\nCode\nlibrary(dplyr)\nset.seed(111)\n# Create random data for 4 groups with specified means\nmy_data <- data.frame(grp = factor(c(rep(c('A', 'B', 'C', 'D'),\n                                         each = 2000))),\n                      val = c(rnorm(2000, 1, .25),\n                              rnorm(2000, 5, .25),\n                              rnorm(2000, 10, .25),\n                              rnorm(2000, 17, .25)))\n\n\nIn this first code block we’ve created four groups with very narrowly defined means. We can extract the means of our simulated data like so:\n\n\nCode\ngroup_means <-\n  my_data |>\n  split(~grp) |> \n  vapply(\\(grp_data) mean(grp_data$val), 1.0, USE.NAMES = TRUE)\n\ngroup_means\n\n\n         A          B          C          D \n 0.9935785  5.0083811 10.0039933 16.9910755 \n\n\nNow what we want to do is run a linear model to compute helmert-coded comparisons. Specifically what we want are these differences:\n\nB - A\nC - mean(B + A)\nD - mean(C + B + A)\n\nWe can compute these differences manually ourselves so we can verify that the model is working as expected.\n\n\n\n\n\n\nExercise 1\n\n\n\nUsing a piece of paper or R, try to manually calculate what the above differences would be. Refer to to code block where we created our toy dataset for the mean values we used for each group. The answers are shown below using the group_means vector we defined.\n\n\n\n\nCode\n# B vs A: mean(B) - mean(A): ~~ 5 - 1 ==> 4\ngroup_means['B'] - group_means['A']\n\n\n       B \n4.014803 \n\n\nCode\n# C vs A+B: mean(C) - mean(A, B): ~~ 10 - mean(1, 5) ==> 7\ngroup_means['C'] - mean(c(group_means['B'],\n                          group_means['A']))\n\n\n       C \n7.003013 \n\n\nCode\n# D vs A+B+C: mean(D) - mean(A, B, C): ~~ 16 - mean(1, 5, 10) ==> 11.67\ngroup_means['D'] - mean(c(group_means['A'],\n                          group_means['B'],\n                          group_means['C']))\n\n\n       D \n11.65576 \n\n\nSo these are the values we should be looking out for in our model. Let’s use the contrastable package (see here) to set contrasts moving forward. This will allow us to set labels easily and swap out contrast schemes.\n\n\nCode\nlibrary(contrastable)\ncoded_data1 <-\n  set_contrasts(my_data, grp ~ contr.helmert | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\nset.seed(111)\nmodel_1_coefs <- coef(lm(val ~ grp, data = coded_data1))\n\nmodel_1_coefs\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    2.007401    2.334338    2.913939 \n\n\nTake a moment to compare the results from the model above to the manual calculations we did. Are these values the same? (no, they are not) All of the values returned by the model are smaller than what we expect. These coefficient values need to be rescaled to get the correct values. If you want to try to figure out how much each value needs to be multiplied yourself, take a moment to compare the manual calculations to the model output.\nWe can scale the values like so:\n\n\nCode\nmodel_1_coefs * (1:4)\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n\n\nSo, the intercept (which is the grand mean) is fine, but the 2nd coefficient was off by a factor of 2, the 3rd by a factor of 3, and the 4th by 4. This is kind of a pain to remember to do, and most people I talk to don’t realize this needs to be done. We can get an idea for a solution from the contrast matrix:\n\n\nCode\ncontr.helmert(4)\n\n\n  [,1] [,2] [,3]\n1   -1   -1   -1\n2    1   -1   -1\n3    0    2   -1\n4    0    0    3\n\n\nFor those just learning about contrast coding (or, if you’re more familiar with advanced topics, try to think of the most basic matrices), it’s a bit surprising to see a value like 3 there. contr.treatment and contr.sum are all 0s, 1s, and -1s, and sometimes you’ll see people use fractions like \\(\\pm0.5\\)– a 3 is a bit out of place. But therein lies the solution: this contrast matrix needs to be scaled. Recall that the coefficients encoding the comparisons were off by factors of 2, 3, and 4. We can scale each column of the matrix using those values:\n\n\nCode\nnew_matrix <- contr.helmert(4)\nnew_matrix[,1] <- new_matrix[,1]/2\nnew_matrix[,2] <- new_matrix[,2]/3\nnew_matrix[,3] <- new_matrix[,3]/4\n\n# Alternatively something like:\n# apply(stats::contr.helmert(4), 2L, \\(x) x / sum(x != 0))\n\nnew_matrix\n\n\n  [,1]       [,2]  [,3]\n1 -0.5 -0.3333333 -0.25\n2  0.5 -0.3333333 -0.25\n3  0.0  0.6666667 -0.25\n4  0.0  0.0000000  0.75\n\n\nWe can use that new matrix to set the contrasts like before, rather than using contr.helmert:\n\n\nCode\ncoded_data2 <- \n  set_contrasts(my_data, grp ~ new_matrix | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\n\nset.seed(111)\nmodel_2_coefs <- coef(lm(val ~ grp, data = coded_data2))\n\nmodel_2_coefs\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n\n\nNow the values are exactly what we expected! But it was a bit annoying to have to remember to either multiply the output values or divide the input values. The contrastable package provides a version of helmert coding that already scaled the matrix appropriately.\n\n\nCode\nhelmert_code(4)\n\n\n     [,1]       [,2]  [,3]\n[1,] -0.5 -0.3333333 -0.25\n[2,]  0.5 -0.3333333 -0.25\n[3,]  0.0  0.6666667 -0.25\n[4,]  0.0  0.0000000  0.75\n\n\nCode\ncoded_data3 <- \n  set_contrasts(my_data, grp ~ helmert_code | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\n\nset.seed(111)\nmodel_3_coefs <- coef(lm(val ~ grp, data = coded_data3))\n\nmodel_3_coefs\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n\n\nAt this point, if you’re a researcher who has used contr.helmert in a published analysis and you didn’t know about all the scaling nonsense, I’m sure your stomach has sunk and your heart rate is elevated. On your mind is probably “but do the p values change??”. They don’t, so take a deep breath and we’ll look at the full model output. Flip through the tabs below and you’ll see that the t values and p values are exactly the same.\n\nWith contr.helmert()With helmert_code()\n\n\n\n\nCode\nset.seed(111)\nsummary(lm(val ~ grp, data = coded_data1))\n\n\n\nCall:\nlm(formula = val ~ grp, data = coded_data1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.93903 -0.16719  0.00158  0.16922  1.02107 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 8.249257   0.002797  2949.3   <2e-16 ***\ngrpAvsB     2.007401   0.003956   507.5   <2e-16 ***\ngrpCvsAB    2.334338   0.002284  1022.1   <2e-16 ***\ngrpDvsABC   2.913939   0.001615  1804.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2502 on 7996 degrees of freedom\nMultiple R-squared:  0.9982,    Adjusted R-squared:  0.9982 \nF-statistic: 1.519e+06 on 3 and 7996 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\nset.seed(111)\nsummary(lm(val ~ grp, data = coded_data3))\n\n\n\nCall:\nlm(formula = val ~ grp, data = coded_data3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.93903 -0.16719  0.00158  0.16922  1.02107 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.249257   0.002797  2949.3   <2e-16 ***\ngrpAvsB      4.014803   0.007911   507.5   <2e-16 ***\ngrpCvsAB     7.003013   0.006851  1022.1   <2e-16 ***\ngrpDvsABC   11.655758   0.006460  1804.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2502 on 7996 degrees of freedom\nMultiple R-squared:  0.9982,    Adjusted R-squared:  0.9982 \nF-statistic: 1.519e+06 on 3 and 7996 DF,  p-value: < 2.2e-16\n\n\n\n\n\nSo the statistical tests are the same yet the coefficients are different. What does this mean for your previous findings? If you concluded an effect with a particular direction based on the sign of the coefficient, then that conclusion still holds.4 The signs of the coefficient values are the same, it’s just the magnitude of the coefficient estimates that’s off when using contr.helmert. However, if you made a claim about the strength of an effect, then you should revisit your analysis. For example, let’s say you run a self paced reading task and conclude that not only are reading times longer in one condition vs the nesting of two others, but in fact the penalty is the same size as some previous effect. You actually probably undersold this result, as it should have been multiplied by three. But on a theoretical basis, the comparison you made to some other effect/process may not hold (because this process is actually 3x more “powerful”).5 Moreover, future work using your effect estimate as a basis for future meta analyses of followup experiments would be a bit misguided. This might look suspicious later on if the experiment is replicated but somehow the effect is three times larger than what you reported."
  },
  {
    "objectID": "posts/helmert/helmert_post.html#helmert-contrasts-and-factor-orders",
    "href": "posts/helmert/helmert_post.html#helmert-contrasts-and-factor-orders",
    "title": "On Helmert Coding",
    "section": "Helmert contrasts and factor orders",
    "text": "Helmert contrasts and factor orders\nHelmert coding provides comparisons with a “nested” structure of the factor levels. The nesting proceeds from the first level towards the last level, but this doesn’t have to be the case.\n\n\n\n\n\n\nExercise 2\n\n\n\nTry running the previous code block using reverse_helmert_code instead of helmert_code. Answer the following questions:\n\nHow did the contrast matrix change?\nHow did the model output change?\nWhat do the coefficients correspond to? Use the approach we did previously with the group_means vector to figure out what difference each comparison corresponds to.\nGiven your observations from the above questions, the labels we used before (c(\"AvsB\", \"CvsAB\", \"DvsABC\")) no longer apply. What other kinds of labels might you use instead? Try adding labels in the set_contrasts() call using the | operator (the label-setting operator).\n\n\n\nIt’s important to remember that R will automatically set the indices of each level to their alphabetical order. We can change this behavior by explicitly setting what the levels of a factor are in the order we want the factor to use. That is, rather than A, B, C, D being assigned numeric indices underlying the factor of 1, 2, 3, 4, we want these assigned indices to be different. If you’re wondering why I wrote the previous clunky sentence, it’s because it’s worth mentioning that R differentiates between unordered and ordered factors. What we want here is NOT an ordered factor with a particular order, but an UNORDERED factor with the levels indexed in a particular order. Ordered factors by default use orthogonal polynomial contrasts (contr.poly), which is not at all what we want right now.6\nWhy does all this alphabetical order-but-not-ordered nonsense matter? Recall that Helmert coding nests from one level to another: either the first to the last or the last to the first. If these are not already in the order we want them to be nested in, then we need to put in some work to match things up like we want. Let’s say we actually want the ordering to be A, C, B, D/ We can set the order by setting the levels parameter of the factor() function.\n\n\nCode\ncoded_data4 <- \n  my_data |> \n  mutate(grp = factor(grp, levels = c(\"A\", \"C\", \"B\", \"D\"))) |> \n  set_contrasts(grp ~ helmert_code)\n\n\nset.seed(111)\nmodel_4_coefs <- coef(lm(val ~ grp, data = coded_data4))\n\nmodel_4_coefs\n\n\n(Intercept)       grp>A       grp>C       grp>B \n  8.2492571   9.0104148  -0.4904048  11.6557579 \n\n\nThe last thing I’ll touch upon is the edge case of contrast coding: factors with only 2 levels. I discuss this at length in my other blog post here, but the main point is that many contrast coding schemes are equivalent to one another when there are only 2 levels. In particular, helmert coding will give you \\(\\pm0.5\\), but so would sum coding that’s been scaled by 2 (contr.sum(2)/2 is not uncommon to see in analysis scripts) or successive difference coding or… many other things. But this equivalence does not hold when there are more than 2 levels. This divergence is why it’s important to be explicit about exactly what your comparisons are trying to describe in the context of the analysis. If the goal is to compare one level to a baseline, then for 2 levels basically any contrast scheme (modulo sign and multiplicative factor) would give you that information. But if that’s the goal for a followup that “just adds another level”, then suddenly using helmert coding vs sum coding will give very different insights for that new comparison."
  },
  {
    "objectID": "posts/helmert/index.html",
    "href": "posts/helmert/index.html",
    "title": "On Helmert Coding",
    "section": "",
    "text": "I’ve had many discussions with colleagues over the past year about helmert contrasts, and I’ve decided to compile some of my notes here. This post discusses helmert coding, which is a type of contrast coding scheme where comparisons are made in a “nesting” fashion. However, there are different ways to represent these nested comparisons in a contrast matrix. Their corresponding hypothesis matrices simplify to the same statistical test, but the coefficient estimates have different magnitudes. These magnitudes differ by a scaling factor, which I will show how to derive. As a result, the statistical inference drawn will be the same whether the matrix is scaled or unscaled, but using the coefficient magnitude at face-value for something else (e.g., claims about differences in reaction time, future power analyses) will be misleading.\nI will include various exercises throughout this document that you can do yourself to better understand what’s going in."
  },
  {
    "objectID": "posts/helmert/index.html#what-is-helmert-coding",
    "href": "posts/helmert/index.html#what-is-helmert-coding",
    "title": "On Helmert Coding",
    "section": "What is Helmert Coding?",
    "text": "What is Helmert Coding?\nHelmert coding is a type of contrast coding scheme that nests levels together for some of the comparisons. This is most evident when using a factor with more than 2 levels.1\nAt an abstract level, let’s say you have four groups A, B, C, and D which allots you 3 comparisons (n-1 degrees of freedom). Helmert coding allows you to compare the means of levels A and B (\\(\\mu_B - \\mu_A\\)) for your first comparison, then the mean of level C compared to the mean of levels A and B (\\(\\mu_C - \\frac{\\mu_A +\\mu_B}{2}\\)), then finally the mean of level D compared to the mean of levels A, B, C (\\(\\mu_D - \\frac{\\mu_A +\\mu_B + \\mu_C}{3}\\)).\nTo give an example, let’s say you’re interested in comparing the duration of d, n, s, and sh word initially in words like dough, no, so, show. We have four levels, so we get three comparisons. We could pick a baseline level and compare the other two levels to it, but there’s a natural structure to these levels: they are all coronal sounds2 but three are continuants and two are sibilants3 (one compact (s) and the other diffuse (sh) ). So, we could compare the two sibilants together (sh-s) and then have another comparison between nasals and sibilants (n-sib=n-(s+sh)), then a final comparison between continuant coronals and a coronal stop. Thus, we have a comparison where two of the comparisons contain nested parts of the data. What we want to see is some statistical test for these comparisons, which themselves are just differences between means. We’ll see that in some versions of Helmert coding, this is a little off.\nHelmert coding is also useful for another reason: the comparisons are mathematically orthogonal to one another, meaning that the comparisons are independent of one another. Read more about this here and here."
  },
  {
    "objectID": "posts/helmert/index.html#issue-with-contr.helmert",
    "href": "posts/helmert/index.html#issue-with-contr.helmert",
    "title": "On Helmert Coding",
    "section": "Issue with contr.helmert",
    "text": "Issue with contr.helmert\nOne issue with the contr.helmert function provided in all installations of R via the stats package though is that the resulting coefficient estimates don’t straightforwardly encode the differences between means. Rather, the results are scaled by some multiplicative factor. We’ll build up to this with a toy example.\n\n\nCode\nlibrary(dplyr)\nset.seed(111)\n# Create random data for 4 groups with specified means\nmy_data <- data.frame(grp = factor(c(rep(c('A', 'B', 'C', 'D'),\n                                         each = 2000))),\n                      val = c(rnorm(2000, 1, .25),\n                              rnorm(2000, 5, .25),\n                              rnorm(2000, 10, .25),\n                              rnorm(2000, 17, .25)))\n\n\nIn this first code block we’ve created four groups with very narrowly defined means. We can extract the means of our simulated data like so:\n\n\nCode\ngroup_means <-\n  my_data |>\n  split(~grp) |> \n  vapply(\\(grp_data) mean(grp_data$val), 1.0, USE.NAMES = TRUE)\n\ngroup_means\n\n\n         A          B          C          D \n 0.9935785  5.0083811 10.0039933 16.9910755 \n\n\nNow what we want to do is run a linear model to compute helmert-coded comparisons. Specifically what we want are these differences:\n\nB - A\nC - mean(B + A)\nD - mean(C + B + A)\n\nWe can compute these differences manually ourselves so we can verify that the model is working as expected.\n\n\n\n\n\n\nExercise 1\n\n\n\nUsing a piece of paper or R, try to manually calculate what the above differences would be. Refer to to code block where we created our toy dataset for the mean values we used for each group. The answers are shown below using the group_means vector we defined.\n\n\n\n\nCode\n# B vs A: mean(B) - mean(A): ~~ 5 - 1 ==> 4\ngroup_means['B'] - group_means['A']\n\n\n       B \n4.014803 \n\n\nCode\n# C vs A+B: mean(C) - mean(A, B): ~~ 10 - mean(1, 5) ==> 7\ngroup_means['C'] - mean(c(group_means['B'],\n                          group_means['A']))\n\n\n       C \n7.003013 \n\n\nCode\n# D vs A+B+C: mean(D) - mean(A, B, C): ~~ 17 - mean(1, 5, 10) ==> 11.67\ngroup_means['D'] - mean(c(group_means['A'],\n                          group_means['B'],\n                          group_means['C']))\n\n\n       D \n11.65576 \n\n\nSo these are the values we should be looking out for in our model. Let’s use the contrastable package (see here) to set contrasts moving forward. This will allow us to set labels easily and swap out contrast schemes.\n\n\nCode\nlibrary(contrastable)\ncoded_data1 <-\n  set_contrasts(my_data, grp ~ contr.helmert | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\nset.seed(111)\nmodel_1_coefs <- coef(lm(val ~ grp, data = coded_data1))\n\nmodel_1_coefs\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    2.007401    2.334338    2.913939 \n\n\nTake a moment to compare the results from the model above to the manual calculations we did. Are these values the same? (no, they are not) All of the values returned by the model are smaller than what we expect. These coefficient values need to be rescaled to get the correct values. If you want to try to figure out how much each value needs to be multiplied yourself, take a moment to compare the manual calculations to the model output.\nWe can scale the values like so:\n\n\nCode\nmodel_1_coefs * (1:4)\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n\n\nSo, the intercept (which is the grand mean) is fine, but the 2nd coefficient was off by a factor of 2, the 3rd by a factor of 3, and the 4th by 4. This is kind of a pain to remember to do, and most people I talk to don’t realize this needs to be done. We can get an idea for a solution from the contrast matrix:\n\n\nCode\ncontr.helmert(4)\n\n\n  [,1] [,2] [,3]\n1   -1   -1   -1\n2    1   -1   -1\n3    0    2   -1\n4    0    0    3\n\n\nFor those just learning about contrast coding (or, if you’re more familiar with advanced topics, try to think of the most basic matrices), it’s a bit surprising to see a value like 3 there. contr.treatment and contr.sum are all 0s, 1s, and -1s, and sometimes you’ll see people use fractions like \\(\\pm0.5\\)– a 3 is a bit out of place. But therein lies the solution: this contrast matrix needs to be scaled. Recall that the coefficients encoding the comparisons were off by factors of 2, 3, and 4. We can scale each column of the matrix using those values:\n\n\nCode\nnew_matrix <- contr.helmert(4)\nnew_matrix[,1] <- new_matrix[,1]/2\nnew_matrix[,2] <- new_matrix[,2]/3\nnew_matrix[,3] <- new_matrix[,3]/4\n\n# Alternatively something like:\n# apply(stats::contr.helmert(4), 2L, \\(x) x / sum(x != 0))\n\nnew_matrix\n\n\n  [,1]       [,2]  [,3]\n1 -0.5 -0.3333333 -0.25\n2  0.5 -0.3333333 -0.25\n3  0.0  0.6666667 -0.25\n4  0.0  0.0000000  0.75\n\n\nWe can use that new matrix to set the contrasts like before, rather than using contr.helmert:\n\n\nCode\ncoded_data2 <- \n  set_contrasts(my_data, grp ~ new_matrix | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\n\nset.seed(111)\nmodel_2_coefs <- coef(lm(val ~ grp, data = coded_data2))\n\nmodel_2_coefs\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n\n\nNow the values are exactly what we expected! But it was a bit annoying to have to remember to either multiply the output values or divide the input values. The contrastable package provides a version of helmert coding that already scaled the matrix appropriately.\n\n\nCode\nhelmert_code(4)\n\n\n     [,1]       [,2]  [,3]\n[1,] -0.5 -0.3333333 -0.25\n[2,]  0.5 -0.3333333 -0.25\n[3,]  0.0  0.6666667 -0.25\n[4,]  0.0  0.0000000  0.75\n\n\nCode\ncoded_data3 <- \n  set_contrasts(my_data, grp ~ helmert_code | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\n\nset.seed(111)\nmodel_3_coefs <- coef(lm(val ~ grp, data = coded_data3))\n\nmodel_3_coefs\n\n\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n\n\nAt this point, if you’re a researcher who has used contr.helmert in a published analysis and you didn’t know about all the scaling nonsense, I’m sure your stomach has sunk and your heart rate is elevated. On your mind is probably “but do the p values change??”. They don’t, so take a deep breath and we’ll look at the full model output. Flip through the tabs below and you’ll see that the t values and p values are exactly the same.\n\nWith contr.helmert()With helmert_code()\n\n\n\n\nCode\nset.seed(111)\nsummary(lm(val ~ grp, data = coded_data1))\n\n\n\nCall:\nlm(formula = val ~ grp, data = coded_data1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.93903 -0.16719  0.00158  0.16922  1.02107 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 8.249257   0.002797  2949.3   <2e-16 ***\ngrpAvsB     2.007401   0.003956   507.5   <2e-16 ***\ngrpCvsAB    2.334338   0.002284  1022.1   <2e-16 ***\ngrpDvsABC   2.913939   0.001615  1804.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2502 on 7996 degrees of freedom\nMultiple R-squared:  0.9982,    Adjusted R-squared:  0.9982 \nF-statistic: 1.519e+06 on 3 and 7996 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\nset.seed(111)\nsummary(lm(val ~ grp, data = coded_data3))\n\n\n\nCall:\nlm(formula = val ~ grp, data = coded_data3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.93903 -0.16719  0.00158  0.16922  1.02107 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.249257   0.002797  2949.3   <2e-16 ***\ngrpAvsB      4.014803   0.007911   507.5   <2e-16 ***\ngrpCvsAB     7.003013   0.006851  1022.1   <2e-16 ***\ngrpDvsABC   11.655758   0.006460  1804.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2502 on 7996 degrees of freedom\nMultiple R-squared:  0.9982,    Adjusted R-squared:  0.9982 \nF-statistic: 1.519e+06 on 3 and 7996 DF,  p-value: < 2.2e-16\n\n\n\n\n\nSo the statistical tests are the same yet the coefficients are different. What does this mean for your previous findings? If you concluded an effect with a particular direction based on the sign of the coefficient, then that conclusion still holds.4 The signs of the coefficient values are the same, it’s just the magnitude of the coefficient estimates that’s off when using contr.helmert. However, if you made a claim about the strength of an effect, then you should revisit your analysis. For example, let’s say you run a self paced reading task and conclude that not only are reading times longer in one condition vs the nesting of two others, but in fact the penalty is the same size as some previous effect. You actually probably undersold this result, as it should have been multiplied by three. But on a theoretical basis, the comparison you made to some other effect/process may not hold (because this process is actually 3x more “powerful”).5 Moreover, future work using your effect estimate as a basis for future meta analyses of followup experiments would be a bit misguided. This might look suspicious later on if the experiment is replicated but somehow the effect is three times larger than what you reported."
  },
  {
    "objectID": "posts/helmert/index.html#helmert-contrasts-and-factor-orders",
    "href": "posts/helmert/index.html#helmert-contrasts-and-factor-orders",
    "title": "On Helmert Coding",
    "section": "Helmert contrasts and factor orders",
    "text": "Helmert contrasts and factor orders\nHelmert coding provides comparisons with a “nested” structure of the factor levels. The nesting proceeds from the first level towards the last level, but this doesn’t have to be the case.\n\n\n\n\n\n\nExercise 2\n\n\n\nTry running the previous code block using reverse_helmert_code instead of helmert_code. Answer the following questions:\n\nHow did the contrast matrix change?\nHow did the model output change?\nWhat do the coefficients correspond to? Use the approach we did previously with the group_means vector to figure out what difference each comparison corresponds to.\nGiven your observations from the above questions, the labels we used before (c(\"AvsB\", \"CvsAB\", \"DvsABC\")) no longer apply. What other kinds of labels might you use instead? Try adding labels in the set_contrasts() call using the | operator (the label-setting operator).\n\n\n\nIt’s important to remember that R will automatically set the indices of each level to their alphabetical order. We can change this behavior by explicitly setting what the levels of a factor are in the order we want the factor to use. That is, rather than A, B, C, D being assigned numeric indices underlying the factor of 1, 2, 3, 4, we want these assigned indices to be different. If you’re wondering why I wrote the previous clunky sentence, it’s because it’s worth mentioning that R differentiates between unordered and ordered factors. What we want here is NOT an ordered factor with a particular order, but an UNORDERED factor with the levels indexed in a particular order. Ordered factors by default use orthogonal polynomial contrasts (contr.poly), which is not at all what we want right now.6\nWhy does all this alphabetical order-but-not-ordered nonsense matter? Recall that Helmert coding nests from one level to another: either the first to the last or the last to the first. If these are not already in the order we want them to be nested in, then we need to put in some work to match things up like we want. Let’s say we actually want the ordering to be A, C, B, D/ We can set the order by setting the levels parameter of the factor() function.\n\n\nCode\ncoded_data4 <- \n  my_data |> \n  mutate(grp = factor(grp, levels = c(\"A\", \"C\", \"B\", \"D\"))) |> \n  set_contrasts(grp ~ helmert_code)\n\n\nset.seed(111)\nmodel_4_coefs <- coef(lm(val ~ grp, data = coded_data4))\n\nmodel_4_coefs\n\n\n(Intercept)       grp>A       grp>C       grp>B \n  8.2492571   9.0104148  -0.4904048  11.6557579 \n\n\nThe last thing I’ll touch upon is the edge case of contrast coding: factors with only 2 levels. I discuss this at length in my other blog post here, but the main point is that many contrast coding schemes are equivalent to one another when there are only 2 levels. In particular, helmert coding will give you \\(\\pm0.5\\), but so would sum coding that’s been scaled by 2 (contr.sum(2)/2 is not uncommon to see in analysis scripts) or successive difference coding or… many other things. But this equivalence does not hold when there are more than 2 levels. This divergence is why it’s important to be explicit about exactly what your comparisons are trying to describe in the context of the analysis. If the goal is to compare one level to a baseline, then for 2 levels basically any contrast scheme (modulo sign and multiplicative factor) would give you that information. But if that’s the goal for a followup that “just adds another level”, then suddenly using helmert coding vs sum coding will give very different insights for that new comparison."
  },
  {
    "objectID": "posts/notefreqs/index.html",
    "href": "posts/notefreqs/index.html",
    "title": "Note Frequencies",
    "section": "",
    "text": "I’ve had Bryan Suits’ website bookmarked for years specifically for the page on converting musical notes to frequencies and wavelengths. I have learned that he has sadly passed away recently, and Michigan Tech removed his website in the process. This is a recreation of the page I had bookmarked."
  },
  {
    "objectID": "posts/notefreqs/index.html#frequencies-for-equal-tempered-scale",
    "href": "posts/notefreqs/index.html#frequencies-for-equal-tempered-scale",
    "title": "Note Frequencies",
    "section": "Frequencies for equal-tempered scale",
    "text": "Frequencies for equal-tempered scale\nSpeed of Sound = 345 m/s = 1130 ft/s = 770 miles/hr. To convert lengths in cm to inches, divide by 2.54.\nSelect a frequency for A4 below. “Middle C” is C4.\n\n440432434436438442444446\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.35\n2109.89\n\n\nC#0/Db0\n17.32\n1991.47\n\n\nD0\n18.35\n1879.69\n\n\nD#0/Eb0\n19.45\n1774.20\n\n\nE0\n20.60\n1674.62\n\n\nF0\n21.83\n1580.63\n\n\nF#0/Gb0\n23.12\n1491.91\n\n\nG0\n24.50\n1408.18\n\n\nG#0/Ab0\n25.96\n1329.14\n\n\nA0\n27.50\n1254.55\n\n\nA#0/Bb0\n29.14\n1184.13\n\n\nB0\n30.87\n1117.67\n\n\nC1\n32.70\n1054.94\n\n\nC#1/Db1\n34.65\n995.73\n\n\nD1\n36.71\n939.85\n\n\nD#1/Eb1\n38.89\n887.10\n\n\nE1\n41.20\n837.31\n\n\nF1\n43.65\n790.31\n\n\nF#1/Gb1\n46.25\n745.96\n\n\nG1\n49.00\n704.09\n\n\nG#1/Ab1\n51.91\n664.57\n\n\nA1\n55.00\n627.27\n\n\nA#1/Bb1\n58.27\n592.07\n\n\nB1\n61.74\n558.84\n\n\nC2\n65.41\n527.47\n\n\nC#2/Db2\n69.30\n497.87\n\n\nD2\n73.42\n469.92\n\n\nD#2/Eb2\n77.78\n443.55\n\n\nE2\n82.41\n418.65\n\n\nF2\n87.31\n395.16\n\n\nF#2/Gb2\n92.50\n372.98\n\n\nG2\n98.00\n352.04\n\n\nG#2/Ab2\n103.83\n332.29\n\n\nA2\n110.00\n313.64\n\n\nA#2/Bb2\n116.54\n296.03\n\n\nB2\n123.47\n279.42\n\n\nC3\n130.81\n263.74\n\n\nC#3/Db3\n138.59\n248.93\n\n\nD3\n146.83\n234.96\n\n\nD#3/Eb3\n155.56\n221.77\n\n\nE3\n164.81\n209.33\n\n\nF3\n174.61\n197.58\n\n\nF#3/Gb3\n185.00\n186.49\n\n\nG3\n196.00\n176.02\n\n\nG#3/Ab3\n207.65\n166.14\n\n\nA3\n220.00\n156.82\n\n\nA#3/Bb3\n233.08\n148.02\n\n\nB3\n246.94\n139.71\n\n\nC4\n261.63\n131.87\n\n\nC#4/Db4\n277.18\n124.47\n\n\nD4\n293.66\n117.48\n\n\nD#4/Eb4\n311.13\n110.89\n\n\nE4\n329.63\n104.66\n\n\nF4\n349.23\n98.79\n\n\nF#4/Gb4\n369.99\n93.24\n\n\nG4\n392.00\n88.01\n\n\nG#4/Ab4\n415.30\n83.07\n\n\nA4\n440.00\n78.41\n\n\nA#4/Bb4\n466.16\n74.01\n\n\nB4\n493.88\n69.85\n\n\nC5\n523.25\n65.93\n\n\nC#5/Db5\n554.37\n62.23\n\n\nD5\n587.33\n58.74\n\n\nD#5/Eb5\n622.25\n55.44\n\n\nE5\n659.26\n52.33\n\n\nF5\n698.46\n49.39\n\n\nF#5/Gb5\n739.99\n46.62\n\n\nG5\n783.99\n44.01\n\n\nG#5/Ab5\n830.61\n41.54\n\n\nA5\n880.00\n39.20\n\n\nA#5/Bb5\n932.33\n37.00\n\n\nB5\n987.77\n34.93\n\n\nC6\n1046.50\n32.97\n\n\nC#6/Db6\n1108.73\n31.12\n\n\nD6\n1174.66\n29.37\n\n\nD#6/Eb6\n1244.51\n27.72\n\n\nE6\n1318.51\n26.17\n\n\nF6\n1396.91\n24.70\n\n\nF#6/Gb6\n1479.98\n23.31\n\n\nG6\n1567.98\n22.00\n\n\nG#6/Ab6\n1661.22\n20.77\n\n\nA6\n1760.00\n19.60\n\n\nA#6/Bb6\n1864.66\n18.50\n\n\nB6\n1975.53\n17.46\n\n\nC7\n2093.00\n16.48\n\n\nC#7/Db7\n2217.46\n15.56\n\n\nD7\n2349.32\n14.69\n\n\nD#7/Eb7\n2489.02\n13.86\n\n\nE7\n2637.02\n13.08\n\n\nF7\n2793.83\n12.35\n\n\nF#7/Gb7\n2959.96\n11.66\n\n\nG7\n3135.96\n11.00\n\n\nG#7/Ab7\n3322.44\n10.38\n\n\nA7\n3520.00\n9.80\n\n\nA#7/Bb7\n3729.31\n9.25\n\n\nB7\n3951.07\n8.73\n\n\nC8\n4186.01\n8.24\n\n\nC#8/Db8\n4434.92\n7.78\n\n\nD8\n4698.64\n7.34\n\n\nD#8/Eb8\n4978.03\n6.93\n\n\nE8\n5274.04\n6.54\n\n\nF8\n5587.65\n6.17\n\n\nF#8/Gb8\n5919.91\n5.83\n\n\nG8\n6271.93\n5.50\n\n\nG#8/Ab8\n6644.88\n5.19\n\n\nA8\n7040.00\n4.90\n\n\nA#8/Bb8\n7458.62\n4.63\n\n\nB8\n7902.13\n4.37\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.05\n2148.96\n\n\nC#0/Db0\n17.01\n2028.35\n\n\nD0\n18.02\n1914.50\n\n\nD#0/Eb0\n19.09\n1807.05\n\n\nE0\n20.23\n1705.63\n\n\nF0\n21.43\n1609.90\n\n\nF#0/Gb0\n22.70\n1519.54\n\n\nG0\n24.05\n1434.26\n\n\nG#0/Ab0\n25.48\n1353.76\n\n\nA0\n27.00\n1277.78\n\n\nA#0/Bb0\n28.61\n1206.06\n\n\nB0\n30.31\n1138.37\n\n\nC1\n32.11\n1074.48\n\n\nC#1/Db1\n34.02\n1014.17\n\n\nD1\n36.04\n957.25\n\n\nD#1/Eb1\n38.18\n903.53\n\n\nE1\n40.45\n852.81\n\n\nF1\n42.86\n804.95\n\n\nF#1/Gb1\n45.41\n759.77\n\n\nG1\n48.11\n717.13\n\n\nG#1/Ab1\n50.97\n676.88\n\n\nA1\n54.00\n638.89\n\n\nA#1/Bb1\n57.21\n603.03\n\n\nB1\n60.61\n569.19\n\n\nC2\n64.22\n537.24\n\n\nC#2/Db2\n68.04\n507.09\n\n\nD2\n72.08\n478.63\n\n\nD#2/Eb2\n76.37\n451.76\n\n\nE2\n80.91\n426.41\n\n\nF2\n85.72\n402.47\n\n\nF#2/Gb2\n90.82\n379.89\n\n\nG2\n96.22\n358.56\n\n\nG#2/Ab2\n101.94\n338.44\n\n\nA2\n108.00\n319.44\n\n\nA#2/Bb2\n114.42\n301.52\n\n\nB2\n121.23\n284.59\n\n\nC3\n128.43\n268.62\n\n\nC#3/Db3\n136.07\n253.54\n\n\nD3\n144.16\n239.31\n\n\nD#3/Eb3\n152.74\n225.88\n\n\nE3\n161.82\n213.20\n\n\nF3\n171.44\n201.24\n\n\nF#3/Gb3\n181.63\n189.94\n\n\nG3\n192.43\n179.28\n\n\nG#3/Ab3\n203.88\n169.22\n\n\nA3\n216.00\n159.72\n\n\nA#3/Bb3\n228.84\n150.76\n\n\nB3\n242.45\n142.30\n\n\nC4\n256.87\n134.31\n\n\nC#4/Db4\n272.14\n126.77\n\n\nD4\n288.33\n119.66\n\n\nD#4/Eb4\n305.47\n112.94\n\n\nE4\n323.63\n106.60\n\n\nF4\n342.88\n100.62\n\n\nF#4/Gb4\n363.27\n94.97\n\n\nG4\n384.87\n89.64\n\n\nG#4/Ab4\n407.75\n84.61\n\n\nA4\n432.00\n79.86\n\n\nA#4/Bb4\n457.69\n75.38\n\n\nB4\n484.90\n71.15\n\n\nC5\n513.74\n67.15\n\n\nC#5/Db5\n544.29\n63.39\n\n\nD5\n576.65\n59.83\n\n\nD#5/Eb5\n610.94\n56.47\n\n\nE5\n647.27\n53.30\n\n\nF5\n685.76\n50.31\n\n\nF#5/Gb5\n726.53\n47.49\n\n\nG5\n769.74\n44.82\n\n\nG#5/Ab5\n815.51\n42.30\n\n\nA5\n864.00\n39.93\n\n\nA#5/Bb5\n915.38\n37.69\n\n\nB5\n969.81\n35.57\n\n\nC6\n1027.47\n33.58\n\n\nC#6/Db6\n1088.57\n31.69\n\n\nD6\n1153.30\n29.91\n\n\nD#6/Eb6\n1221.88\n28.24\n\n\nE6\n1294.54\n26.65\n\n\nF6\n1371.51\n25.15\n\n\nF#6/Gb6\n1453.07\n23.74\n\n\nG6\n1539.47\n22.41\n\n\nG#6/Ab6\n1631.01\n21.15\n\n\nA6\n1728.00\n19.97\n\n\nA#6/Bb6\n1830.75\n18.84\n\n\nB6\n1939.61\n17.79\n\n\nC7\n2054.95\n16.79\n\n\nC#7/Db7\n2177.14\n15.85\n\n\nD7\n2306.60\n14.96\n\n\nD#7/Eb7\n2443.76\n14.12\n\n\nE7\n2589.07\n13.33\n\n\nF7\n2743.03\n12.58\n\n\nF#7/Gb7\n2906.14\n11.87\n\n\nG7\n3078.95\n11.21\n\n\nG#7/Ab7\n3262.03\n10.58\n\n\nA7\n3456.00\n9.98\n\n\nA#7/Bb7\n3661.50\n9.42\n\n\nB7\n3879.23\n8.89\n\n\nC8\n4109.90\n8.39\n\n\nC#8/Db8\n4354.29\n7.92\n\n\nD8\n4613.21\n7.48\n\n\nD#8/Eb8\n4887.52\n7.06\n\n\nE8\n5178.15\n6.66\n\n\nF8\n5486.06\n6.29\n\n\nF#8/Gb8\n5812.28\n5.94\n\n\nG8\n6157.89\n5.60\n\n\nG#8/Ab8\n6524.06\n5.29\n\n\nA8\n6912.00\n4.99\n\n\nA#8/Bb8\n7323.01\n4.71\n\n\nB8\n7758.46\n4.45\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.13\n2139.05\n\n\nC#0/Db0\n17.09\n2019.00\n\n\nD0\n18.10\n1905.68\n\n\nD#0/Eb0\n19.18\n1798.72\n\n\nE0\n20.32\n1697.77\n\n\nF0\n21.53\n1602.48\n\n\nF#0/Gb0\n22.81\n1512.54\n\n\nG0\n24.17\n1427.65\n\n\nG#0/Ab0\n25.60\n1347.52\n\n\nA0\n27.12\n1271.89\n\n\nA#0/Bb0\n28.74\n1200.50\n\n\nB0\n30.45\n1133.12\n\n\nC1\n32.26\n1069.53\n\n\nC#1/Db1\n34.18\n1009.50\n\n\nD1\n36.21\n952.84\n\n\nD#1/Eb1\n38.36\n899.36\n\n\nE1\n40.64\n848.88\n\n\nF1\n43.06\n801.24\n\n\nF#1/Gb1\n45.62\n756.27\n\n\nG1\n48.33\n713.82\n\n\nG#1/Ab1\n51.21\n673.76\n\n\nA1\n54.25\n635.94\n\n\nA#1/Bb1\n57.48\n600.25\n\n\nB1\n60.89\n566.56\n\n\nC2\n64.51\n534.76\n\n\nC#2/Db2\n68.35\n504.75\n\n\nD2\n72.42\n476.42\n\n\nD#2/Eb2\n76.72\n449.68\n\n\nE2\n81.28\n424.44\n\n\nF2\n86.12\n400.62\n\n\nF#2/Gb2\n91.24\n378.13\n\n\nG2\n96.66\n356.91\n\n\nG#2/Ab2\n102.41\n336.88\n\n\nA2\n108.50\n317.97\n\n\nA#2/Bb2\n114.95\n300.13\n\n\nB2\n121.79\n283.28\n\n\nC3\n129.03\n267.38\n\n\nC#3/Db3\n136.70\n252.37\n\n\nD3\n144.83\n238.21\n\n\nD#3/Eb3\n153.44\n224.84\n\n\nE3\n162.57\n212.22\n\n\nF3\n172.23\n200.31\n\n\nF#3/Gb3\n182.47\n189.07\n\n\nG3\n193.33\n178.46\n\n\nG#3/Ab3\n204.82\n168.44\n\n\nA3\n217.00\n158.99\n\n\nA#3/Bb3\n229.90\n150.06\n\n\nB3\n243.57\n141.64\n\n\nC4\n258.06\n133.69\n\n\nC#4/Db4\n273.40\n126.19\n\n\nD4\n289.66\n119.11\n\n\nD#4/Eb4\n306.88\n112.42\n\n\nE4\n325.13\n106.11\n\n\nF4\n344.47\n100.16\n\n\nF#4/Gb4\n364.95\n94.53\n\n\nG4\n386.65\n89.23\n\n\nG#4/Ab4\n409.64\n84.22\n\n\nA4\n434.00\n79.49\n\n\nA#4/Bb4\n459.81\n75.03\n\n\nB4\n487.15\n70.82\n\n\nC5\n516.12\n66.85\n\n\nC#5/Db5\n546.81\n63.09\n\n\nD5\n579.32\n59.55\n\n\nD#5/Eb5\n613.77\n56.21\n\n\nE5\n650.27\n53.06\n\n\nF5\n688.93\n50.08\n\n\nF#5/Gb5\n729.90\n47.27\n\n\nG5\n773.30\n44.61\n\n\nG#5/Ab5\n819.28\n42.11\n\n\nA5\n868.00\n39.75\n\n\nA#5/Bb5\n919.61\n37.52\n\n\nB5\n974.30\n35.41\n\n\nC6\n1032.23\n33.42\n\n\nC#6/Db6\n1093.61\n31.55\n\n\nD6\n1158.64\n29.78\n\n\nD#6/Eb6\n1227.54\n28.11\n\n\nE6\n1300.53\n26.53\n\n\nF6\n1377.86\n25.04\n\n\nF#6/Gb6\n1459.80\n23.63\n\n\nG6\n1546.60\n22.31\n\n\nG#6/Ab6\n1638.57\n21.05\n\n\nA6\n1736.00\n19.87\n\n\nA#6/Bb6\n1839.23\n18.76\n\n\nB6\n1948.59\n17.71\n\n\nC7\n2064.46\n16.71\n\n\nC#7/Db7\n2187.22\n15.77\n\n\nD7\n2317.28\n14.89\n\n\nD#7/Eb7\n2455.07\n14.05\n\n\nE7\n2601.06\n13.26\n\n\nF7\n2755.73\n12.52\n\n\nF#7/Gb7\n2919.59\n11.82\n\n\nG7\n3093.20\n11.15\n\n\nG#7/Ab7\n3277.13\n10.53\n\n\nA7\n3472.00\n9.94\n\n\nA#7/Bb7\n3678.46\n9.38\n\n\nB7\n3897.19\n8.85\n\n\nC8\n4128.93\n8.36\n\n\nC#8/Db8\n4374.45\n7.89\n\n\nD8\n4634.56\n7.44\n\n\nD#8/Eb8\n4910.15\n7.03\n\n\nE8\n5202.12\n6.63\n\n\nF8\n5511.46\n6.26\n\n\nF#8/Gb8\n5839.18\n5.91\n\n\nG8\n6186.40\n5.58\n\n\nG#8/Ab8\n6554.26\n5.26\n\n\nA8\n6944.00\n4.97\n\n\nA#8/Bb8\n7356.91\n4.69\n\n\nB8\n7794.38\n4.43\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.20\n2129.24\n\n\nC#0/Db0\n17.17\n2009.74\n\n\nD0\n18.19\n1896.94\n\n\nD#0/Eb0\n19.27\n1790.47\n\n\nE0\n20.41\n1689.98\n\n\nF0\n21.63\n1595.13\n\n\nF#0/Gb0\n22.91\n1505.60\n\n\nG0\n24.28\n1421.10\n\n\nG#0/Ab0\n25.72\n1341.34\n\n\nA0\n27.25\n1266.06\n\n\nA#0/Bb0\n28.87\n1195.00\n\n\nB0\n30.59\n1127.93\n\n\nC1\n32.41\n1064.62\n\n\nC#1/Db1\n34.33\n1004.87\n\n\nD1\n36.37\n948.47\n\n\nD#1/Eb1\n38.54\n895.24\n\n\nE1\n40.83\n844.99\n\n\nF1\n43.26\n797.56\n\n\nF#1/Gb1\n45.83\n752.80\n\n\nG1\n48.55\n710.55\n\n\nG#1/Ab1\n51.44\n670.67\n\n\nA1\n54.50\n633.03\n\n\nA#1/Bb1\n57.74\n597.50\n\n\nB1\n61.17\n563.96\n\n\nC2\n64.81\n532.31\n\n\nC#2/Db2\n68.67\n502.43\n\n\nD2\n72.75\n474.23\n\n\nD#2/Eb2\n77.07\n447.62\n\n\nE2\n81.66\n422.50\n\n\nF2\n86.51\n398.78\n\n\nF#2/Gb2\n91.66\n376.40\n\n\nG2\n97.11\n355.27\n\n\nG#2/Ab2\n102.88\n335.33\n\n\nA2\n109.00\n316.51\n\n\nA#2/Bb2\n115.48\n298.75\n\n\nB2\n122.35\n281.98\n\n\nC3\n129.62\n266.16\n\n\nC#3/Db3\n137.33\n251.22\n\n\nD3\n145.50\n237.12\n\n\nD#3/Eb3\n154.15\n223.81\n\n\nE3\n163.32\n211.25\n\n\nF3\n173.03\n199.39\n\n\nF#3/Gb3\n183.32\n188.20\n\n\nG3\n194.22\n177.64\n\n\nG#3/Ab3\n205.76\n167.67\n\n\nA3\n218.00\n158.26\n\n\nA#3/Bb3\n230.96\n149.37\n\n\nB3\n244.70\n140.99\n\n\nC4\n259.25\n133.08\n\n\nC#4/Db4\n274.66\n125.61\n\n\nD4\n291.00\n118.56\n\n\nD#4/Eb4\n308.30\n111.90\n\n\nE4\n326.63\n105.62\n\n\nF4\n346.05\n99.70\n\n\nF#4/Gb4\n366.63\n94.10\n\n\nG4\n388.43\n88.82\n\n\nG#4/Ab4\n411.53\n83.83\n\n\nA4\n436.00\n79.13\n\n\nA#4/Bb4\n461.93\n74.69\n\n\nB4\n489.39\n70.50\n\n\nC5\n518.49\n66.54\n\n\nC#5/Db5\n549.33\n62.80\n\n\nD5\n581.99\n59.28\n\n\nD#5/Eb5\n616.60\n55.95\n\n\nE5\n653.26\n52.81\n\n\nF5\n692.11\n49.85\n\n\nF#5/Gb5\n733.26\n47.05\n\n\nG5\n776.86\n44.41\n\n\nG#5/Ab5\n823.06\n41.92\n\n\nA5\n872.00\n39.56\n\n\nA#5/Bb5\n923.85\n37.34\n\n\nB5\n978.79\n35.25\n\n\nC6\n1036.99\n33.27\n\n\nC#6/Db6\n1098.65\n31.40\n\n\nD6\n1163.98\n29.64\n\n\nD#6/Eb6\n1233.19\n27.98\n\n\nE6\n1306.52\n26.41\n\n\nF6\n1384.21\n24.92\n\n\nF#6/Gb6\n1466.52\n23.53\n\n\nG6\n1553.73\n22.20\n\n\nG#6/Ab6\n1646.12\n20.96\n\n\nA6\n1744.00\n19.78\n\n\nA#6/Bb6\n1847.70\n18.67\n\n\nB6\n1957.57\n17.62\n\n\nC7\n2073.98\n16.63\n\n\nC#7/Db7\n2197.30\n15.70\n\n\nD7\n2327.96\n14.82\n\n\nD#7/Eb7\n2466.39\n13.99\n\n\nE7\n2613.05\n13.20\n\n\nF7\n2768.43\n12.46\n\n\nF#7/Gb7\n2933.05\n11.76\n\n\nG7\n3107.45\n11.10\n\n\nG#7/Ab7\n3292.23\n10.48\n\n\nA7\n3488.00\n9.89\n\n\nA#7/Bb7\n3695.41\n9.34\n\n\nB7\n3915.15\n8.81\n\n\nC8\n4147.95\n8.32\n\n\nC#8/Db8\n4394.60\n7.85\n\n\nD8\n4655.92\n7.41\n\n\nD#8/Eb8\n4932.78\n6.99\n\n\nE8\n5226.10\n6.60\n\n\nF8\n5536.85\n6.23\n\n\nF#8/Gb8\n5866.09\n5.88\n\n\nG8\n6214.91\n5.55\n\n\nG#8/Ab8\n6584.47\n5.24\n\n\nA8\n6976.00\n4.95\n\n\nA#8/Bb8\n7390.81\n4.67\n\n\nB8\n7830.30\n4.41\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.28\n2119.52\n\n\nC#0/Db0\n17.25\n2000.56\n\n\nD0\n18.27\n1888.28\n\n\nD#0/Eb0\n19.36\n1782.30\n\n\nE0\n20.51\n1682.26\n\n\nF0\n21.73\n1587.85\n\n\nF#0/Gb0\n23.02\n1498.73\n\n\nG0\n24.39\n1414.61\n\n\nG#0/Ab0\n25.84\n1335.21\n\n\nA0\n27.38\n1260.27\n\n\nA#0/Bb0\n29.00\n1189.54\n\n\nB0\n30.73\n1122.78\n\n\nC1\n32.55\n1059.76\n\n\nC#1/Db1\n34.49\n1000.28\n\n\nD1\n36.54\n944.14\n\n\nD#1/Eb1\n38.71\n891.15\n\n\nE1\n41.02\n841.13\n\n\nF1\n43.46\n793.92\n\n\nF#1/Gb1\n46.04\n749.36\n\n\nG1\n48.78\n707.30\n\n\nG#1/Ab1\n51.68\n667.61\n\n\nA1\n54.75\n630.14\n\n\nA#1/Bb1\n58.01\n594.77\n\n\nB1\n61.45\n561.39\n\n\nC2\n65.11\n529.88\n\n\nC#2/Db2\n68.98\n500.14\n\n\nD2\n73.08\n472.07\n\n\nD#2/Eb2\n77.43\n445.57\n\n\nE2\n82.03\n420.57\n\n\nF2\n86.91\n396.96\n\n\nF#2/Gb2\n92.08\n374.68\n\n\nG2\n97.55\n353.65\n\n\nG#2/Ab2\n103.35\n333.80\n\n\nA2\n109.50\n315.07\n\n\nA#2/Bb2\n116.01\n297.39\n\n\nB2\n122.91\n280.69\n\n\nC3\n130.22\n264.94\n\n\nC#3/Db3\n137.96\n250.07\n\n\nD3\n146.16\n236.03\n\n\nD#3/Eb3\n154.86\n222.79\n\n\nE3\n164.06\n210.28\n\n\nF3\n173.82\n198.48\n\n\nF#3/Gb3\n184.16\n187.34\n\n\nG3\n195.11\n176.83\n\n\nG#3/Ab3\n206.71\n166.90\n\n\nA3\n219.00\n157.53\n\n\nA#3/Bb3\n232.02\n148.69\n\n\nB3\n245.82\n140.35\n\n\nC4\n260.44\n132.47\n\n\nC#4/Db4\n275.92\n125.04\n\n\nD4\n292.33\n118.02\n\n\nD#4/Eb4\n309.71\n111.39\n\n\nE4\n328.13\n105.14\n\n\nF4\n347.64\n99.24\n\n\nF#4/Gb4\n368.31\n93.67\n\n\nG4\n390.21\n88.41\n\n\nG#4/Ab4\n413.42\n83.45\n\n\nA4\n438.00\n78.77\n\n\nA#4/Bb4\n464.04\n74.35\n\n\nB4\n491.64\n70.17\n\n\nC5\n520.87\n66.23\n\n\nC#5/Db5\n551.85\n62.52\n\n\nD5\n584.66\n59.01\n\n\nD#5/Eb5\n619.43\n55.70\n\n\nE5\n656.26\n52.57\n\n\nF5\n695.28\n49.62\n\n\nF#5/Gb5\n736.63\n46.84\n\n\nG5\n780.43\n44.21\n\n\nG#5/Ab5\n826.83\n41.73\n\n\nA5\n876.00\n39.38\n\n\nA#5/Bb5\n928.09\n37.17\n\n\nB5\n983.28\n35.09\n\n\nC6\n1041.75\n33.12\n\n\nC#6/Db6\n1103.69\n31.26\n\n\nD6\n1169.32\n29.50\n\n\nD#6/Eb6\n1238.85\n27.85\n\n\nE6\n1312.52\n26.29\n\n\nF6\n1390.56\n24.81\n\n\nF#6/Gb6\n1473.25\n23.42\n\n\nG6\n1560.85\n22.10\n\n\nG#6/Ab6\n1653.67\n20.86\n\n\nA6\n1752.00\n19.69\n\n\nA#6/Bb6\n1856.18\n18.59\n\n\nB6\n1966.55\n17.54\n\n\nC7\n2083.49\n16.56\n\n\nC#7/Db7\n2207.38\n15.63\n\n\nD7\n2338.64\n14.75\n\n\nD#7/Eb7\n2477.70\n13.92\n\n\nE7\n2625.03\n13.14\n\n\nF7\n2781.13\n12.41\n\n\nF#7/Gb7\n2946.50\n11.71\n\n\nG7\n3121.71\n11.05\n\n\nG#7/Ab7\n3307.34\n10.43\n\n\nA7\n3504.00\n9.85\n\n\nA#7/Bb7\n3712.36\n9.29\n\n\nB7\n3933.11\n8.77\n\n\nC8\n4166.98\n8.28\n\n\nC#8/Db8\n4414.76\n7.81\n\n\nD8\n4677.28\n7.38\n\n\nD#8/Eb8\n4955.40\n6.96\n\n\nE8\n5250.07\n6.57\n\n\nF8\n5562.25\n6.20\n\n\nF#8/Gb8\n5893.00\n5.85\n\n\nG8\n6243.42\n5.53\n\n\nG#8/Ab8\n6614.67\n5.22\n\n\nA8\n7008.00\n4.92\n\n\nA#8/Bb8\n7424.72\n4.65\n\n\nB8\n7866.21\n4.39\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.43\n2100.34\n\n\nC#0/Db0\n17.40\n1982.46\n\n\nD0\n18.44\n1871.19\n\n\nD#0/Eb0\n19.53\n1766.17\n\n\nE0\n20.70\n1667.04\n\n\nF0\n21.93\n1573.48\n\n\nF#0/Gb0\n23.23\n1485.16\n\n\nG0\n24.61\n1401.81\n\n\nG#0/Ab0\n26.07\n1323.13\n\n\nA0\n27.62\n1248.87\n\n\nA#0/Bb0\n29.27\n1178.78\n\n\nB0\n31.01\n1112.62\n\n\nC1\n32.85\n1050.17\n\n\nC#1/Db1\n34.81\n991.23\n\n\nD1\n36.87\n935.59\n\n\nD#1/Eb1\n39.07\n883.08\n\n\nE1\n41.39\n833.52\n\n\nF1\n43.85\n786.74\n\n\nF#1/Gb1\n46.46\n742.58\n\n\nG1\n49.22\n700.90\n\n\nG#1/Ab1\n52.15\n661.57\n\n\nA1\n55.25\n624.43\n\n\nA#1/Bb1\n58.54\n589.39\n\n\nB1\n62.02\n556.31\n\n\nC2\n65.70\n525.08\n\n\nC#2/Db2\n69.61\n495.61\n\n\nD2\n73.75\n467.80\n\n\nD#2/Eb2\n78.14\n441.54\n\n\nE2\n82.78\n416.76\n\n\nF2\n87.70\n393.37\n\n\nF#2/Gb2\n92.92\n371.29\n\n\nG2\n98.44\n350.45\n\n\nG#2/Ab2\n104.30\n330.78\n\n\nA2\n110.50\n312.22\n\n\nA#2/Bb2\n117.07\n294.69\n\n\nB2\n124.03\n278.15\n\n\nC3\n131.41\n262.54\n\n\nC#3/Db3\n139.22\n247.81\n\n\nD3\n147.50\n233.90\n\n\nD#3/Eb3\n156.27\n220.77\n\n\nE3\n165.56\n208.38\n\n\nF3\n175.41\n196.68\n\n\nF#3/Gb3\n185.84\n185.65\n\n\nG3\n196.89\n175.23\n\n\nG#3/Ab3\n208.60\n165.39\n\n\nA3\n221.00\n156.11\n\n\nA#3/Bb3\n234.14\n147.35\n\n\nB3\n248.06\n139.08\n\n\nC4\n262.81\n131.27\n\n\nC#4/Db4\n278.44\n123.90\n\n\nD4\n295.00\n116.95\n\n\nD#4/Eb4\n312.54\n110.39\n\n\nE4\n331.13\n104.19\n\n\nF4\n350.82\n98.34\n\n\nF#4/Gb4\n371.68\n92.82\n\n\nG4\n393.78\n87.61\n\n\nG#4/Ab4\n417.19\n82.70\n\n\nA4\n442.00\n78.05\n\n\nA#4/Bb4\n468.28\n73.67\n\n\nB4\n496.13\n69.54\n\n\nC5\n525.63\n65.64\n\n\nC#5/Db5\n556.89\n61.95\n\n\nD5\n590.00\n58.47\n\n\nD#5/Eb5\n625.08\n55.19\n\n\nE5\n662.25\n52.09\n\n\nF5\n701.63\n49.17\n\n\nF#5/Gb5\n743.35\n46.41\n\n\nG5\n787.55\n43.81\n\n\nG#5/Ab5\n834.38\n41.35\n\n\nA5\n884.00\n39.03\n\n\nA#5/Bb5\n936.57\n36.84\n\n\nB5\n992.26\n34.77\n\n\nC6\n1051.26\n32.82\n\n\nC#6/Db6\n1113.77\n30.98\n\n\nD6\n1180.00\n29.24\n\n\nD#6/Eb6\n1250.16\n27.60\n\n\nE6\n1324.50\n26.05\n\n\nF6\n1403.26\n24.59\n\n\nF#6/Gb6\n1486.70\n23.21\n\n\nG6\n1575.11\n21.90\n\n\nG#6/Ab6\n1668.77\n20.67\n\n\nA6\n1768.00\n19.51\n\n\nA#6/Bb6\n1873.13\n18.42\n\n\nB6\n1984.51\n17.38\n\n\nC7\n2102.52\n16.41\n\n\nC#7/Db7\n2227.54\n15.49\n\n\nD7\n2360.00\n14.62\n\n\nD#7/Eb7\n2500.33\n13.80\n\n\nE7\n2649.01\n13.02\n\n\nF7\n2806.53\n12.29\n\n\nF#7/Gb7\n2973.41\n11.60\n\n\nG7\n3150.22\n10.95\n\n\nG#7/Ab7\n3337.54\n10.34\n\n\nA7\n3536.00\n9.76\n\n\nA#7/Bb7\n3746.26\n9.21\n\n\nB7\n3969.03\n8.69\n\n\nC8\n4205.04\n8.20\n\n\nC#8/Db8\n4455.08\n7.74\n\n\nD8\n4719.99\n7.31\n\n\nD#8/Eb8\n5000.66\n6.90\n\n\nE8\n5298.01\n6.51\n\n\nF8\n5613.05\n6.15\n\n\nF#8/Gb8\n5946.82\n5.80\n\n\nG8\n6300.44\n5.48\n\n\nG#8/Ab8\n6675.08\n5.17\n\n\nA8\n7072.00\n4.88\n\n\nA#8/Bb8\n7492.52\n4.60\n\n\nB8\n7938.05\n4.35\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.50\n2090.88\n\n\nC#0/Db0\n17.48\n1973.53\n\n\nD0\n18.52\n1862.76\n\n\nD#0/Eb0\n19.62\n1758.21\n\n\nE0\n20.79\n1659.53\n\n\nF0\n22.03\n1566.39\n\n\nF#0/Gb0\n23.33\n1478.47\n\n\nG0\n24.72\n1395.49\n\n\nG#0/Ab0\n26.19\n1317.17\n\n\nA0\n27.75\n1243.24\n\n\nA#0/Bb0\n29.40\n1173.47\n\n\nB0\n31.15\n1107.60\n\n\nC1\n33.00\n1045.44\n\n\nC#1/Db1\n34.96\n986.76\n\n\nD1\n37.04\n931.38\n\n\nD#1/Eb1\n39.24\n879.11\n\n\nE1\n41.58\n829.77\n\n\nF1\n44.05\n783.19\n\n\nF#1/Gb1\n46.67\n739.24\n\n\nG1\n49.44\n697.75\n\n\nG#1/Ab1\n52.39\n658.59\n\n\nA1\n55.50\n621.62\n\n\nA#1/Bb1\n58.80\n586.73\n\n\nB1\n62.30\n553.80\n\n\nC2\n66.00\n522.72\n\n\nC#2/Db2\n69.93\n493.38\n\n\nD2\n74.08\n465.69\n\n\nD#2/Eb2\n78.49\n439.55\n\n\nE2\n83.16\n414.88\n\n\nF2\n88.10\n391.60\n\n\nF#2/Gb2\n93.34\n369.62\n\n\nG2\n98.89\n348.87\n\n\nG#2/Ab2\n104.77\n329.29\n\n\nA2\n111.00\n310.81\n\n\nA#2/Bb2\n117.60\n293.37\n\n\nB2\n124.59\n276.90\n\n\nC3\n132.00\n261.36\n\n\nC#3/Db3\n139.85\n246.69\n\n\nD3\n148.17\n232.85\n\n\nD#3/Eb3\n156.98\n219.78\n\n\nE3\n166.31\n207.44\n\n\nF3\n176.20\n195.80\n\n\nF#3/Gb3\n186.68\n184.81\n\n\nG3\n197.78\n174.44\n\n\nG#3/Ab3\n209.54\n164.65\n\n\nA3\n222.00\n155.41\n\n\nA#3/Bb3\n235.20\n146.68\n\n\nB3\n249.19\n138.45\n\n\nC4\n264.00\n130.68\n\n\nC#4/Db4\n279.70\n123.35\n\n\nD4\n296.33\n116.42\n\n\nD#4/Eb4\n313.96\n109.89\n\n\nE4\n332.62\n103.72\n\n\nF4\n352.40\n97.90\n\n\nF#4/Gb4\n373.36\n92.40\n\n\nG4\n395.56\n87.22\n\n\nG#4/Ab4\n419.08\n82.32\n\n\nA4\n444.00\n77.70\n\n\nA#4/Bb4\n470.40\n73.34\n\n\nB4\n498.37\n69.23\n\n\nC5\n528.01\n65.34\n\n\nC#5/Db5\n559.40\n61.67\n\n\nD5\n592.67\n58.21\n\n\nD#5/Eb5\n627.91\n54.94\n\n\nE5\n665.25\n51.86\n\n\nF5\n704.81\n48.95\n\n\nF#5/Gb5\n746.72\n46.20\n\n\nG5\n791.12\n43.61\n\n\nG#5/Ab5\n838.16\n41.16\n\n\nA5\n888.00\n38.85\n\n\nA#5/Bb5\n940.80\n36.67\n\n\nB5\n996.75\n34.61\n\n\nC6\n1056.02\n32.67\n\n\nC#6/Db6\n1118.81\n30.84\n\n\nD6\n1185.34\n29.11\n\n\nD#6/Eb6\n1255.82\n27.47\n\n\nE6\n1330.50\n25.93\n\n\nF6\n1409.61\n24.47\n\n\nF#6/Gb6\n1493.43\n23.10\n\n\nG6\n1582.24\n21.80\n\n\nG#6/Ab6\n1676.32\n20.58\n\n\nA6\n1776.00\n19.43\n\n\nA#6/Bb6\n1881.61\n18.34\n\n\nB6\n1993.49\n17.31\n\n\nC7\n2112.03\n16.33\n\n\nC#7/Db7\n2237.62\n15.42\n\n\nD7\n2370.68\n14.55\n\n\nD#7/Eb7\n2511.64\n13.74\n\n\nE7\n2660.99\n12.97\n\n\nF7\n2819.22\n12.24\n\n\nF#7/Gb7\n2986.86\n11.55\n\n\nG7\n3164.47\n10.90\n\n\nG#7/Ab7\n3352.64\n10.29\n\n\nA7\n3552.00\n9.71\n\n\nA#7/Bb7\n3763.21\n9.17\n\n\nB7\n3986.99\n8.65\n\n\nC8\n4224.06\n8.17\n\n\nC#8/Db8\n4475.24\n7.71\n\n\nD8\n4741.35\n7.28\n\n\nD#8/Eb8\n5023.29\n6.87\n\n\nE8\n5321.99\n6.48\n\n\nF8\n5638.45\n6.12\n\n\nF#8/Gb8\n5973.73\n5.78\n\n\nG8\n6328.94\n5.45\n\n\nG#8/Ab8\n6705.28\n5.15\n\n\nA8\n7104.00\n4.86\n\n\nA#8/Bb8\n7526.43\n4.58\n\n\nB8\n7973.97\n4.33\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\nFrequency (Hz)\nWavelength (cm)\n\n\n\n\nC0\n16.57\n2081.50\n\n\nC#0/Db0\n17.56\n1964.68\n\n\nD0\n18.60\n1854.41\n\n\nD#0/Eb0\n19.71\n1750.33\n\n\nE0\n20.88\n1652.09\n\n\nF0\n22.12\n1559.36\n\n\nF#0/Gb0\n23.44\n1471.84\n\n\nG0\n24.83\n1389.24\n\n\nG#0/Ab0\n26.31\n1311.26\n\n\nA0\n27.88\n1237.67\n\n\nA#0/Bb0\n29.53\n1168.20\n\n\nB0\n31.29\n1102.64\n\n\nC1\n33.15\n1040.75\n\n\nC#1/Db1\n35.12\n982.34\n\n\nD1\n37.21\n927.20\n\n\nD#1/Eb1\n39.42\n875.16\n\n\nE1\n41.77\n826.04\n\n\nF1\n44.25\n779.68\n\n\nF#1/Gb1\n46.88\n735.92\n\n\nG1\n49.67\n694.62\n\n\nG#1/Ab1\n52.62\n655.63\n\n\nA1\n55.75\n618.83\n\n\nA#1/Bb1\n59.07\n584.10\n\n\nB1\n62.58\n551.32\n\n\nC2\n66.30\n520.38\n\n\nC#2/Db2\n70.24\n491.17\n\n\nD2\n74.42\n463.60\n\n\nD#2/Eb2\n78.84\n437.58\n\n\nE2\n83.53\n413.02\n\n\nF2\n88.50\n389.84\n\n\nF#2/Gb2\n93.76\n367.96\n\n\nG2\n99.34\n347.31\n\n\nG#2/Ab2\n105.24\n327.82\n\n\nA2\n111.50\n309.42\n\n\nA#2/Bb2\n118.13\n292.05\n\n\nB2\n125.15\n275.66\n\n\nC3\n132.60\n260.19\n\n\nC#3/Db3\n140.48\n245.58\n\n\nD3\n148.83\n231.80\n\n\nD#3/Eb3\n157.68\n218.79\n\n\nE3\n167.06\n206.51\n\n\nF3\n177.00\n194.92\n\n\nF#3/Gb3\n187.52\n183.98\n\n\nG3\n198.67\n173.65\n\n\nG#3/Ab3\n210.48\n163.91\n\n\nA3\n223.00\n154.71\n\n\nA#3/Bb3\n236.26\n146.03\n\n\nB3\n250.31\n137.83\n\n\nC4\n265.19\n130.09\n\n\nC#4/Db4\n280.96\n122.79\n\n\nD4\n297.67\n115.90\n\n\nD#4/Eb4\n315.37\n109.40\n\n\nE4\n334.12\n103.26\n\n\nF4\n353.99\n97.46\n\n\nF#4/Gb4\n375.04\n91.99\n\n\nG4\n397.34\n86.83\n\n\nG#4/Ab4\n420.97\n81.95\n\n\nA4\n446.00\n77.35\n\n\nA#4/Bb4\n472.52\n73.01\n\n\nB4\n500.62\n68.91\n\n\nC5\n530.39\n65.05\n\n\nC#5/Db5\n561.92\n61.40\n\n\nD5\n595.34\n57.95\n\n\nD#5/Eb5\n630.74\n54.70\n\n\nE5\n668.24\n51.63\n\n\nF5\n707.98\n48.73\n\n\nF#5/Gb5\n750.08\n46.00\n\n\nG5\n794.68\n43.41\n\n\nG#5/Ab5\n841.94\n40.98\n\n\nA5\n892.00\n38.68\n\n\nA#5/Bb5\n945.04\n36.51\n\n\nB5\n1001.24\n34.46\n\n\nC6\n1060.77\n32.52\n\n\nC#6/Db6\n1123.85\n30.70\n\n\nD6\n1190.68\n28.98\n\n\nD#6/Eb6\n1261.48\n27.35\n\n\nE6\n1336.49\n25.81\n\n\nF6\n1415.96\n24.37\n\n\nF#6/Gb6\n1500.16\n23.00\n\n\nG6\n1589.36\n21.71\n\n\nG#6/Ab6\n1683.87\n20.49\n\n\nA6\n1784.00\n19.34\n\n\nA#6/Bb6\n1890.08\n18.25\n\n\nB6\n2002.47\n17.23\n\n\nC7\n2121.55\n16.26\n\n\nC#7/Db7\n2247.70\n15.35\n\n\nD7\n2381.35\n14.49\n\n\nD#7/Eb7\n2522.96\n13.67\n\n\nE7\n2672.98\n12.91\n\n\nF7\n2831.92\n12.18\n\n\nF#7/Gb7\n3000.32\n11.50\n\n\nG7\n3178.73\n10.85\n\n\nG#7/Ab7\n3367.74\n10.24\n\n\nA7\n3568.00\n9.67\n\n\nA#7/Bb7\n3780.16\n9.13\n\n\nB7\n4004.94\n8.61\n\n\nC8\n4243.09\n8.13\n\n\nC#8/Db8\n4495.40\n7.67\n\n\nD8\n4762.71\n7.24\n\n\nD#8/Eb8\n5045.91\n6.84\n\n\nE8\n5345.96\n6.45\n\n\nF8\n5663.85\n6.09\n\n\nF#8/Gb8\n6000.64\n5.75\n\n\nG8\n6357.45\n5.43\n\n\nG#8/Ab8\n6735.49\n5.12\n\n\nA8\n7136.00\n4.83\n\n\nA#8/Bb8\n7560.33\n4.56\n\n\nB8\n8009.89\n4.31\n\n\n\n\n\n\n\n\n\n\n\nWayback machine links below:\n\nScales: Just vs Equal Temperament (and related topics)\nEquations\nNote Frequency Table\nMore about Speed of Sound.\n\n\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22631)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] gt_0.8.0              sosprosody_0.0.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.10       rstudioapi_0.14   knitr_1.42        magrittr_2.0.3   \n [5] munsell_0.5.0     tidyselect_1.2.0  colorspace_2.0-3  R6_2.5.1         \n [9] rlang_1.1.2       fastmap_1.1.1     fansi_1.0.3       dplyr_1.1.0      \n[13] tools_4.2.2       grid_4.2.2        gtable_0.3.1      data.table_1.14.8\n[17] xfun_0.37         utf8_1.2.2        cli_3.6.2         withr_2.5.0      \n[21] htmltools_0.5.7   yaml_2.3.7        digest_0.6.33     tibble_3.1.8     \n[25] lifecycle_1.0.4   ggplot2_3.4.0     sass_0.4.5        htmlwidgets_1.6.1\n[29] vctrs_0.6.5       glue_1.6.2        evaluate_0.21     rmarkdown_2.20   \n[33] compiler_4.2.2    pillar_1.9.0      scales_1.2.1      generics_0.1.3   \n[37] jsonlite_1.8.4    pkgconfig_2.0.3"
  },
  {
    "objectID": "posts/notefreqs/index.html#section",
    "href": "posts/notefreqs/index.html#section",
    "title": "Note Frequencies",
    "section": "432",
    "text": "432\n\n\n\n\n\n\n\n\n\nNote\nFrequency\nWavelength\n\n\n\n\nC0\n16.05\n2148.96\n\n\nC#0/Db0\n17.01\n2028.35\n\n\nD0\n18.02\n1914.50\n\n\nD#0/Eb0\n19.09\n1807.05\n\n\nE0\n20.23\n1705.63\n\n\nF0\n21.43\n1609.90\n\n\nF#0/Gb0\n22.70\n1519.54\n\n\nG0\n24.05\n1434.26\n\n\nG#0/Ab0\n25.48\n1353.76\n\n\nA0\n27.00\n1277.78\n\n\nA#0/Bb0\n28.61\n1206.06\n\n\nB0\n30.31\n1138.37\n\n\nC1\n32.11\n1074.48\n\n\nC#1/Db1\n34.02\n1014.17\n\n\nD1\n36.04\n957.25\n\n\nD#1/Eb1\n38.18\n903.53\n\n\nE1\n40.45\n852.81\n\n\nF1\n42.86\n804.95\n\n\nF#1/Gb1\n45.41\n759.77\n\n\nG1\n48.11\n717.13\n\n\nG#1/Ab1\n50.97\n676.88\n\n\nA1\n54.00\n638.89\n\n\nA#1/Bb1\n57.21\n603.03\n\n\nB1\n60.61\n569.19\n\n\nC2\n64.22\n537.24\n\n\nC#2/Db2\n68.04\n507.09\n\n\nD2\n72.08\n478.63\n\n\nD#2/Eb2\n76.37\n451.76\n\n\nE2\n80.91\n426.41\n\n\nF2\n85.72\n402.47\n\n\nF#2/Gb2\n90.82\n379.89\n\n\nG2\n96.22\n358.56\n\n\nG#2/Ab2\n101.94\n338.44\n\n\nA2\n108.00\n319.44\n\n\nA#2/Bb2\n114.42\n301.52\n\n\nB2\n121.23\n284.59\n\n\nC3\n128.43\n268.62\n\n\nC#3/Db3\n136.07\n253.54\n\n\nD3\n144.16\n239.31\n\n\nD#3/Eb3\n152.74\n225.88\n\n\nE3\n161.82\n213.20\n\n\nF3\n171.44\n201.24\n\n\nF#3/Gb3\n181.63\n189.94\n\n\nG3\n192.43\n179.28\n\n\nG#3/Ab3\n203.88\n169.22\n\n\nA3\n216.00\n159.72\n\n\nA#3/Bb3\n228.84\n150.76\n\n\nB3\n242.45\n142.30\n\n\nC4\n256.87\n134.31\n\n\nC#4/Db4\n272.14\n126.77\n\n\nD4\n288.33\n119.66\n\n\nD#4/Eb4\n305.47\n112.94\n\n\nE4\n323.63\n106.60\n\n\nF4\n342.88\n100.62\n\n\nF#4/Gb4\n363.27\n94.97\n\n\nG4\n384.87\n89.64\n\n\nG#4/Ab4\n407.75\n84.61\n\n\nA4\n432.00\n79.86\n\n\nA#4/Bb4\n457.69\n75.38\n\n\nB4\n484.90\n71.15\n\n\nC5\n513.74\n67.15\n\n\nC#5/Db5\n544.29\n63.39\n\n\nD5\n576.65\n59.83\n\n\nD#5/Eb5\n610.94\n56.47\n\n\nE5\n647.27\n53.30\n\n\nF5\n685.76\n50.31\n\n\nF#5/Gb5\n726.53\n47.49\n\n\nG5\n769.74\n44.82\n\n\nG#5/Ab5\n815.51\n42.30\n\n\nA5\n864.00\n39.93\n\n\nA#5/Bb5\n915.38\n37.69\n\n\nB5\n969.81\n35.57\n\n\nC6\n1027.47\n33.58\n\n\nC#6/Db6\n1088.57\n31.69\n\n\nD6\n1153.30\n29.91\n\n\nD#6/Eb6\n1221.88\n28.24\n\n\nE6\n1294.54\n26.65\n\n\nF6\n1371.51\n25.15\n\n\nF#6/Gb6\n1453.07\n23.74\n\n\nG6\n1539.47\n22.41\n\n\nG#6/Ab6\n1631.01\n21.15\n\n\nA6\n1728.00\n19.97\n\n\nA#6/Bb6\n1830.75\n18.84\n\n\nB6\n1939.61\n17.79\n\n\nC7\n2054.95\n16.79\n\n\nC#7/Db7\n2177.14\n15.85\n\n\nD7\n2306.60\n14.96\n\n\nD#7/Eb7\n2443.76\n14.12\n\n\nE7\n2589.07\n13.33\n\n\nF7\n2743.03\n12.58\n\n\nF#7/Gb7\n2906.14\n11.87\n\n\nG7\n3078.95\n11.21\n\n\nG#7/Ab7\n3262.03\n10.58\n\n\nA7\n3456.00\n9.98\n\n\nA#7/Bb7\n3661.50\n9.42\n\n\nB7\n3879.23\n8.89\n\n\nC8\n4109.90\n8.39\n\n\nC#8/Db8\n4354.29\n7.92\n\n\nD8\n4613.21\n7.48\n\n\nD#8/Eb8\n4887.52\n7.06\n\n\nE8\n5178.15\n6.66\n\n\nF8\n5486.06\n6.29\n\n\nF#8/Gb8\n5812.28\n5.94\n\n\nG8\n6157.89\n5.60\n\n\nG#8/Ab8\n6524.06\n5.29\n\n\nA8\n6912.00\n4.99\n\n\nA#8/Bb8\n7323.01\n4.71\n\n\nB8\n7758.46\n4.45"
  },
  {
    "objectID": "posts/notefreqs/index.html#section-1",
    "href": "posts/notefreqs/index.html#section-1",
    "title": "Note Frequencies",
    "section": "434",
    "text": "434"
  },
  {
    "objectID": "posts/notefreqs/index.html#section-2",
    "href": "posts/notefreqs/index.html#section-2",
    "title": "Note Frequencies",
    "section": "436",
    "text": "436"
  },
  {
    "objectID": "posts/notefreqs/index.html#section-3",
    "href": "posts/notefreqs/index.html#section-3",
    "title": "Note Frequencies",
    "section": "438",
    "text": "438"
  },
  {
    "objectID": "posts/notefreqs/index.html#section-4",
    "href": "posts/notefreqs/index.html#section-4",
    "title": "Note Frequencies",
    "section": "440",
    "text": "440"
  },
  {
    "objectID": "posts/notefreqs/index.html#section-5",
    "href": "posts/notefreqs/index.html#section-5",
    "title": "Note Frequencies",
    "section": "442",
    "text": "442"
  },
  {
    "objectID": "posts/notefreqs/index.html#section-6",
    "href": "posts/notefreqs/index.html#section-6",
    "title": "Note Frequencies",
    "section": "444",
    "text": "444"
  },
  {
    "objectID": "posts/notefreqs/index.html#section-7",
    "href": "posts/notefreqs/index.html#section-7",
    "title": "Note Frequencies",
    "section": "446",
    "text": "446"
  }
]