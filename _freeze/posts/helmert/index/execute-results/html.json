{
  "hash": "2dd431ae228cd20b999badd80f987449",
  "result": {
    "markdown": "---\ntitle: \"On Helmert Coding\"\nformat: html\nself-contained: true\nauthor: \n - name: Thomas Sostarics\n - url: https://tsostarics.com/\n - affiliation: Northwestern University Department of Linguistics\n - orcid: 0000-0002-1178-7967\ndate: '2024-01-14'\ncitation:\n  url: https://tsostaricsblog.netlify.app/posts/helmert\neditor: source\ndescription: \"This post discusses the behavior of Helmert contrasts.\"\ntoc: true\ncategories: ['R']\ncode-tools: true\ncode-fold: show\n---\n\n\nI've had many discussions with colleagues over the past year about helmert \ncontrasts, and I've decided to compile some of my notes here.\nThis post discusses helmert coding, which is a type of contrast coding scheme where comparisons are made in a \"nesting\" fashion.\nHowever, there are different ways to represent these nested comparisons in a contrast matrix.\nTheir corresponding hypothesis matrices simplify to the same statistical test, but the coefficient estimates have different magnitudes.\nThese magnitudes differ by a scaling factor, which I will show how to derive.\nAs a result, the statistical inference drawn will be the same whether the matrix is scaled or unscaled, but using the coefficient magnitude at face-value for something else (e.g., claims about differences in reaction time, future power analyses) will be misleading.\n\nI will include various exercises throughout this document that you can do yourself\nto better understand what's going in.\n\n\n## What is Helmert Coding?\n\nHelmert coding is a type of contrast coding scheme that *nests levels together* for some of the comparisons.\nThis is most evident when using a factor with more than 2 levels.[^1]\n\nAt an abstract level, let's say you have four groups `A`, `B`, `C`, and `D` which allots you 3 comparisons (`n-1` degrees of freedom).\nHelmert coding allows you to compare the means of levels `A` and `B` ($\\mu_B - \\mu_A$) for your first comparison, then the mean of level `C` compared to the mean of levels `A` and `B` ($\\mu_C - \\frac{\\mu_A +\\mu_B}{2}$), then finally the mean of level `D` compared to the mean of levels `A`, `B`, `C` ($\\mu_D - \\frac{\\mu_A +\\mu_B + \\mu_C}{3}$).\n\nTo give an example, let's say you're interested in comparing the duration of `d`, `n`, `s`, and `sh` word initially in words like *dough, no, so, show*.\nWe have four levels, so we get three comparisons.\nWe *could* pick a baseline level and compare the other two levels to it, but there's a natural *structure* to these levels: they are all coronal sounds[^2] but three are continuants and two are sibilants[^3] (one compact (s) and the other diffuse (sh) ).\nSo, we could compare the two sibilants together (`sh-s`) and then have another comparison between nasals and sibilants (`n-sib=n-(s+sh)`), then a final comparison between continuant coronals and a coronal stop.\nThus, we have a comparison where two of the comparisons contain nested parts of the data.\nWhat we want to see is some statistical test for these comparisons, which themselves are just differences between means.\nWe'll see that in some versions of Helmert coding, this is a little off.\n\n\n[^1]: See my other post on contrast coding [here](https://tsostaricsblog.netlify.app/posts/contrastable/), where I show how many contrast coding schemes yield the same contrast matrix for 2-level factors.\n[^2]: Coronal sounds are made with tongue constrictions somewhere between the alveolar ridge (the bump behind your teeth) and the hard palate.\n[^3]: Sibilants are s-like sounds, in English we have s (as in hisssssss) and sh (as in wissssshhhhh). S is *compact* in that its energy is compacted to a high frequency band around 8-10kHz, while sh is *diffuse* because its energy is spread out across a larger frequency range, making it sound more \"fuzzy\" like white noise.\n\n\nHelmert coding is also useful for another reason: the comparisons are mathematically *orthogonal* to one another, meaning that the comparisons are independent of one another. \nRead more about this [here](https://www.southampton.ac.uk/~cpd/anovas/datasets/Orthogonal%20contrasts.htm#:~:text=Orthogonal%20contrasts,at%20least%20three%20fixed%20levels.) and [here](https://online.stat.psu.edu/stat505/lesson/8/8.6).\n\n## Issue with contr.helmert\n\nOne issue with the `contr.helmert` function provided in all installations of R\nvia the stats package though is that the resulting coefficient estimates don't\nstraightforwardly encode the differences between means.\nRather, the results are *scaled* by some multiplicative factor.\nWe'll build up to this with a toy example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nset.seed(111)\n# Create random data for 4 groups with specified means\nmy_data <- data.frame(grp = factor(c(rep(c('A', 'B', 'C', 'D'),\n                                         each = 2000))),\n                      val = c(rnorm(2000, 1, .25),\n                              rnorm(2000, 5, .25),\n                              rnorm(2000, 10, .25),\n                              rnorm(2000, 17, .25)))\n```\n:::\n\n\nIn this first code block we've created four groups with very narrowly defined\nmeans. \nWe can extract the means of our simulated data like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroup_means <-\n  my_data |>\n  split(~grp) |> \n  vapply(\\(grp_data) mean(grp_data$val), 1.0, USE.NAMES = TRUE)\n\ngroup_means\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         A          B          C          D \n 0.9935785  5.0083811 10.0039933 16.9910755 \n```\n:::\n:::\n\n\nNow what we want to do is run a linear model to compute helmert-coded comparisons.\nSpecifically what we want are these differences:\n\n - `B - A`\n - `C - mean(B + A)`\n - `D - mean(C + B + A)`\n \n \nWe can compute these differences manually ourselves so we can verify that the\nmodel is working as expected.\n\n:::{.callout-note}\n\n## ***Exercise 1***\n\nUsing a piece of paper or R, try to manually calculate what the above\ndifferences would be.\nRefer to to code block where we created our toy dataset for the mean values\nwe used for each group.\nThe answers are shown below using the `group_means` vector we defined.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# B vs A: mean(B) - mean(A): ~~ 5 - 1 ==> 4\ngroup_means['B'] - group_means['A']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       B \n4.014803 \n```\n:::\n\n```{.r .cell-code}\n# C vs A+B: mean(C) - mean(A, B): ~~ 10 - mean(1, 5) ==> 7\ngroup_means['C'] - mean(c(group_means['B'],\n                          group_means['A']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       C \n7.003013 \n```\n:::\n\n```{.r .cell-code}\n# D vs A+B+C: mean(D) - mean(A, B, C): ~~ 17 - mean(1, 5, 10) ==> 11.67\ngroup_means['D'] - mean(c(group_means['A'],\n                          group_means['B'],\n                          group_means['C']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       D \n11.65576 \n```\n:::\n:::\n\n\nSo these are the values we should be looking out for in our model.\nLet's use the `contrastable` package ([see here](https://github.com/tsostarics/contrastable)) \nto set contrasts moving forward.\nThis will allow us to set labels easily and swap out contrast schemes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(contrastable)\ncoded_data1 <-\n  set_contrasts(my_data, grp ~ contr.helmert | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\nset.seed(111)\nmodel_1_coefs <- coef(lm(val ~ grp, data = coded_data1))\n\nmodel_1_coefs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    2.007401    2.334338    2.913939 \n```\n:::\n:::\n\n\nTake a moment to compare the results from the model above to the manual \ncalculations we did.\nAre these values the same? (no, they are not)\nAll of the values returned by the model are smaller than what we expect.\nThese coefficient values need to be rescaled to get the correct values.\nIf you want to try to figure out how much each value needs to be multiplied\nyourself, take a moment to compare the manual calculations to the model output.\n\nWe can scale the values like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_1_coefs * (1:4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n```\n:::\n:::\n\n\nSo, the intercept (which is the grand mean) is fine, but the 2nd coefficient\nwas off by a factor of 2, the 3rd by a factor of 3, and the 4th by 4.\nThis is kind of a pain to remember to do, and most people I talk to don't\nrealize this needs to be done.\nWe can get an idea for a solution from the contrast matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontr.helmert(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1] [,2] [,3]\n1   -1   -1   -1\n2    1   -1   -1\n3    0    2   -1\n4    0    0    3\n```\n:::\n:::\n\n\nFor those just learning about contrast coding (or, if you're more familiar with\nadvanced topics, try to think of the most basic matrices), it's a bit surprising\nto see a value like 3 there.\n`contr.treatment` and `contr.sum` are all 0s, 1s, and -1s, and sometimes you'll \nsee people use fractions like $\\pm0.5$-- a 3 is a bit out of place.\nBut therein lies the solution: this contrast matrix needs to be scaled.\nRecall that the coefficients encoding the comparisons were off by factors of\n2, 3, and 4.\nWe can scale each column of the matrix using those values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_matrix <- contr.helmert(4)\nnew_matrix[,1] <- new_matrix[,1]/2\nnew_matrix[,2] <- new_matrix[,2]/3\nnew_matrix[,3] <- new_matrix[,3]/4\n\n# Alternatively something like:\n# apply(stats::contr.helmert(4), 2L, \\(x) x / sum(x != 0))\n\nnew_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1]       [,2]  [,3]\n1 -0.5 -0.3333333 -0.25\n2  0.5 -0.3333333 -0.25\n3  0.0  0.6666667 -0.25\n4  0.0  0.0000000  0.75\n```\n:::\n:::\n\n\nWe can use that new matrix to set the contrasts like before, rather than\nusing `contr.helmert`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoded_data2 <- \n  set_contrasts(my_data, grp ~ new_matrix | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\n\nset.seed(111)\nmodel_2_coefs <- coef(lm(val ~ grp, data = coded_data2))\n\nmodel_2_coefs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n```\n:::\n:::\n\n\nNow the values are exactly what we expected!\nBut it was a bit annoying to have to remember to either multiply the output\nvalues or divide the input values.\nThe `contrastable` package provides a version of helmert coding that already\nscaled the matrix appropriately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhelmert_code(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]       [,2]  [,3]\n[1,] -0.5 -0.3333333 -0.25\n[2,]  0.5 -0.3333333 -0.25\n[3,]  0.0  0.6666667 -0.25\n[4,]  0.0  0.0000000  0.75\n```\n:::\n\n```{.r .cell-code}\ncoded_data3 <- \n  set_contrasts(my_data, grp ~ helmert_code | c(\"AvsB\", \"CvsAB\", \"DvsABC\"))\n\n\nset.seed(111)\nmodel_3_coefs <- coef(lm(val ~ grp, data = coded_data3))\n\nmodel_3_coefs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)     grpAvsB    grpCvsAB   grpDvsABC \n   8.249257    4.014803    7.003013   11.655758 \n```\n:::\n:::\n\n\nAt this point, if you're a researcher who has used `contr.helmert` in a\npublished analysis and you didn't know about all the scaling nonsense, I'm sure \nyour stomach has sunk and your heart rate is elevated.\nOn your mind is probably *\"but do the p values change??\"*.\nThey don't, so take a deep breath and we'll look at the full model output.\nFlip through the tabs below and you'll see that the t values and p values\nare exactly the same.\n\n::: {.panel-tabset}\n\n## With `contr.helmert()`\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(111)\nsummary(lm(val ~ grp, data = coded_data1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = val ~ grp, data = coded_data1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.93903 -0.16719  0.00158  0.16922  1.02107 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 8.249257   0.002797  2949.3   <2e-16 ***\ngrpAvsB     2.007401   0.003956   507.5   <2e-16 ***\ngrpCvsAB    2.334338   0.002284  1022.1   <2e-16 ***\ngrpDvsABC   2.913939   0.001615  1804.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2502 on 7996 degrees of freedom\nMultiple R-squared:  0.9982,\tAdjusted R-squared:  0.9982 \nF-statistic: 1.519e+06 on 3 and 7996 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## With `helmert_code()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(111)\nsummary(lm(val ~ grp, data = coded_data3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = val ~ grp, data = coded_data3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.93903 -0.16719  0.00158  0.16922  1.02107 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.249257   0.002797  2949.3   <2e-16 ***\ngrpAvsB      4.014803   0.007911   507.5   <2e-16 ***\ngrpCvsAB     7.003013   0.006851  1022.1   <2e-16 ***\ngrpDvsABC   11.655758   0.006460  1804.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2502 on 7996 degrees of freedom\nMultiple R-squared:  0.9982,\tAdjusted R-squared:  0.9982 \nF-statistic: 1.519e+06 on 3 and 7996 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n:::\n\nSo the statistical tests are the same yet the coefficients are different.\nWhat does this mean for your previous findings?\nIf you concluded an effect with a particular direction based on the sign of\nthe coefficient, then that conclusion still holds.[^4]\nThe signs of the coefficient values are the same, it's just the magnitude of\nthe coefficient estimates that's off when using `contr.helmert`.\nHowever, if you made a claim about the **strength** of an effect, then you\nshould revisit your analysis.\nFor example, let's say you run a self paced reading task and conclude that\nnot only are reading times longer in one condition vs the nesting of two others,\nbut in fact the penalty is the same size as some previous effect.\nYou actually probably undersold this result, as it should have been multiplied\nby three.\nBut on a theoretical basis, the comparison you made to some other effect/process\nmay not hold (because this process is actually 3x more \"powerful\").[^5]\nMoreover, future work using your effect estimate as a basis for future\nmeta analyses of followup experiments would be a bit misguided.\nThis might look suspicious later on if the experiment is replicated but somehow \nthe  effect is three times larger than what you reported.\n\n [^4]: Let's withhold meta-level discussion about what constitutes an effect,\n decisions based on p-value criteria, replicability, etc. This is all outside\n the scope of this post.\n [^5]: Let's say the coefficient you got is `+0.05` on the log scale, which \n is equivalent to a `5.1%` slowdown between conditions. If the effect is \n actually supposed to be `+0.15`, it's still a slowdown but it's actually a\n `16.1%` slowdown!\n \n## Helmert contrasts and factor orders\n\nHelmert coding provides comparisons with a \"nested\" structure of the factor\nlevels.\nThe nesting proceeds from the first level towards the last level, but this\ndoesn't have to be the case.\n\n:::{.callout-note}\n\n## ***Exercise 2***\n\nTry running the previous code block using `reverse_helmert_code` instead of\n`helmert_code`.\nAnswer the following questions:\n\n 1. How did the contrast matrix change?\n 2. How did the model output change?\n 3. What do the coefficients correspond to? Use the approach we did previously\n with the `group_means` vector to figure out what difference each comparison\n corresponds to.\n 4. Given your observations from the above questions, the labels we used before\n (`c(\"AvsB\", \"CvsAB\", \"DvsABC\")`) no longer apply. What other kinds of labels\n might you use instead? Try adding labels in the `set_contrasts()` call using\n the `|` operator (the label-setting operator).\n\n:::\n\nIt's important to remember that R will automatically set the indices of each\nlevel to their alphabetical order.\nWe can change this behavior by explicitly setting what the levels of a factor are\n*in the order we want the factor to use*.\nThat is, rather than `A`, `B`, `C`, `D` being assigned numeric indices underlying\nthe factor of 1, 2, 3, 4, we want these assigned indices to be different.\nIf you're wondering why I wrote the previous clunky sentence, it's because it's\nworth mentioning that R differentiates between unordered and ordered factors.\nWhat we want here is NOT an ordered factor with a particular order, but\nan UNORDERED factor with the levels indexed in a particular order.\nOrdered factors by default use orthogonal polynomial contrasts (`contr.poly`),\nwhich is not at all what we want right now.[^6]\n\n\n [^6]: You can see what the default contrasts are using `options()$contrasts`\n\nWhy does all this alphabetical order-but-not-*ordered* nonsense matter?\nRecall that Helmert coding nests from one level to another: either the\nfirst to the last or the last to the first.\nIf these are not already in the order we want them to be nested in, then we need\nto put in some work to match things up like we want.\nLet's say we actually want the ordering to be `A`, `C`, `B`, `D`/\nWe can set the order by setting the `levels` parameter of the `factor()` \nfunction.\n \n\n::: {.cell}\n\n```{.r .cell-code}\ncoded_data4 <- \n  my_data |> \n  mutate(grp = factor(grp, levels = c(\"A\", \"C\", \"B\", \"D\"))) |> \n  set_contrasts(grp ~ helmert_code)\n\n\nset.seed(111)\nmodel_4_coefs <- coef(lm(val ~ grp, data = coded_data4))\n\nmodel_4_coefs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)       grp>A       grp>C       grp>B \n  8.2492571   9.0104148  -0.4904048  11.6557579 \n```\n:::\n:::\n\n\nThe last thing I'll touch upon is the edge case of contrast coding: factors\nwith only 2 levels.\nI discuss this at length in my other blog post [here](https://tsostaricsblog.netlify.app/posts/contrastable/), but the main point\nis that many contrast coding schemes are equivalent to one another when\nthere are only 2 levels.\nIn particular, helmert coding will give you $\\pm0.5$, but so would sum coding\nthat's been scaled by 2 (`contr.sum(2)/2` is not uncommon to see in analysis scripts)\nor successive difference coding or... many other things.\nBut this equivalence does not hold when there are more than 2 levels.\nThis divergence is why it's important to be explicit about exactly what your\ncomparisons are trying to describe in the context of the analysis.\nIf the goal is to compare one level to a baseline, then for 2 levels basically\nany contrast scheme (modulo sign and multiplicative factor) would give you that \ninformation.\nBut if that's the goal for a followup that \"just adds another level\", then suddenly\nusing helmert coding vs sum coding will give very different insights for that new\ncomparison.\n\n# Takeaways\n\n - Helmert coding is useful for categorical variables where the comparisons have\n a nested structure to them.\n - The coefficients of `contr.helmert` need to be scaled to recover the actual\n differences of interest.\n - The statistical tests for scaled and unscaled helmert matrices are exactly\n the same: the differences are only in the magnitude of the effects.\n - Care should be taken in correctly setting up the nesting structure.\n \n\n:::{.callout-note}\n\n## ***Final Exercise***\n\n 1. Select another type of contrast coding scheme other than Helmert coding,\n for example, `contr.sum`. Compare the matrices for 2 levels and for 4 levels.\n How do the matrices differ? What kinds of comparisons are encoded by the\n scheme you selected?\n 2. Consider your field or your own research. Come up with an example of a\n categorical variable that can be construed as having a nested structure. How\n should the levels be ordered to get the nesting structure right? Come up with\n fake means for each level; can you predict what the correct signs for the\n coefficients would be? See the intro for an example from segmental phonology.\n 3. A researcher has written the following sentence in their paper:\n \"We fit a linear mixed effects model in R to our data, with all categorical\n variables using helmert coding.\"\n Given the examples from this post and your own observations, what additional\n information would you want the researcher to share in order to be able to\n interpret their results correctly? Moreover, *where* would you want this\n information to be shared? In the preceding sentence? In the following sentence?\n An appendix? A footnote?\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}